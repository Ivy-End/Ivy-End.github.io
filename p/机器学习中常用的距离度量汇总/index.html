<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合"><title>机器学习中常用的距离度量汇总</title>
<link rel=canonical href=https://ivy-end.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/><link rel=stylesheet href=/scss/style.min.67c1e6a52965b9c1077a0e7417d06a71e09b317e03ba9c912269013f124c7982.css><meta property='og:title' content="机器学习中常用的距离度量汇总"><meta property='og:description' content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合"><meta property='og:url' content='https://ivy-end.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/'><meta property='og:site_name' content='退思轩'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='神经网络'><meta property='article:tag' content='测度论'><meta property='article:tag' content='空间距离'><meta property='article:tag' content='字符距离'><meta property='article:tag' content='集合距离'><meta property='article:tag' content='分布距离'><meta property='article:published_time' content='2023-08-18T11:27:00+08:00'><meta property='article:modified_time' content='2023-08-18T11:27:00+08:00'><meta name=twitter:title content="机器学习中常用的距离度量汇总"><meta name=twitter:description content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu190e528798e90195d3cefef01f0a2d44_410_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🎓</span></figure><div class=site-meta><h1 class=site-name><a href=/>退思轩</a></h1><h2 class=site-description>Carpe diem!</h2></div></header><ol class=menu-social><li><a href=mailto:suda.kwang@gmail.com target=_blank title=Mail rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-mail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="M3 7l9 6 9-6"/></svg></a></li><li><a href=https://github.com/Ivy-End target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://space.bilibili.com/39142721 target=_blank title=Bilibili rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-bilibili"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 10a4 4 0 014-4h10a4 4 0 014 4v6a4 4 0 01-4 4H7a4 4 0 01-4-4v-6z"/><path d="M8 3l2 3"/><path d="M16 3l-2 3"/><path d="M9 13v-2"/><path d="M15 11v2"/></svg></a></li><li><a href=https://ivy-end.github.io/index.xml target=_blank title=RSS rel=me><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/%E5%85%B3%E4%BA%8E/><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-user"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 7a4 4 0 108 0A4 4 0 008 7"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#距离的定义>距离的定义</a></li><li><a href=#空间距离>空间距离</a><ol><li><a href=#欧几里得距离euclidean-distance>欧几里得距离（Euclidean Distance）</a></li><li><a href=#曼哈顿距离manhattan-distance>曼哈顿距离（Manhattan Distance）</a></li><li><a href=#切比雪夫距离chebyschev-distance>切比雪夫距离（Chebyschev Distance）</a></li><li><a href=#闵可夫斯基距离minkowski-distance>闵可夫斯基距离（Minkowski Distance）</a></li><li><a href=#标准化欧几里得距离standardized-euclidean-distance>标准化欧几里得距离（Standardized Euclidean Distance）</a></li><li><a href=#马氏距离mahalanobis-distance>马氏距离（Mahalanobis Distance）</a></li><li><a href=#兰氏距离lance-and-williams-distance>兰氏距离（Lance and Williams Distance）</a></li><li><a href=#余弦相似度cosine-similarity>余弦相似度（Cosine Similarity）</a></li><li><a href=#测地距离geodesic-distance>测地距离（Geodesic Distance）</a></li><li><a href=#布雷柯蒂斯距离bray-curtis-distance>布雷柯蒂斯距离（Bray Curtis Distance）</a></li><li><a href=#半正矢距离haversine-distance>半正矢距离（Haversine Distance）</a></li></ol></li><li><a href=#字符距离>字符距离</a><ol><li><a href=#汉明距离hamming-distance>汉明距离（Hamming Distance）</a></li><li><a href=#莱文斯坦距离levenshtein-distance>莱文斯坦距离（Levenshtein Distance）</a></li><li><a href=#归一化-google-距离normalized-google-distance>归一化 Google 距离（Normalized Google Distance）</a></li><li><a href=#jaro-winkler-相似度jaro-winkler-similarity>Jaro-Winkler 相似度（<strong>J</strong>aro-Winkler Sim<strong>ilarity</strong>）</a></li><li><a href=#李距离lee-distance>李距离（Lee Distance）</a></li></ol></li><li><a href=#集合距离>集合距离</a><ol><li><a href=#杰卡德相似系数jaccard-similarity-coefficient>杰卡德相似系数（Jaccard Similarity Coefficient）</a></li><li><a href=#奥奇亚系数ochiia-coefficient>奥奇亚系数（Ochiia Coefficient）</a></li><li><a href=#戴斯系数dice-coefficient>戴斯系数（Dice Coefficient）</a></li><li><a href=#豪斯多夫距离hausdorff-distance>豪斯多夫距离（Hausdorff Distance）</a></li></ol></li><li><a href=#分布距离>分布距离</a><ol><li><a href=#皮尔逊相关系数pearson-correlation-coefficient>皮尔逊相关系数（Pearson Correlation Coefficient）</a></li><li><a href=#卡方度量chi-square-measure>卡方度量（Chi-square Measure）</a></li><li><a href=#交叉熵cross-entropy>交叉熵（Cross Entropy）</a></li><li><a href=#kl-散度kullback-leibler-divergence>KL 散度（Kullback-Leibler Divergence）</a></li><li><a href=#js-散度jensen-shannon-divergence>JS 散度（Jensen-Shannon Divergence）</a></li><li><a href=#海林格距离hellinger-distance>海林格距离（Hellinger Distance）</a></li><li><a href=#α-散度α-divergence>α 散度（α Divergence）</a></li><li><a href=#f-散度f-divergence>F 散度（F Divergence）</a></li><li><a href=#布雷格曼散度bregman-divergence>布雷格曼散度（Bregman Divergence）</a></li><li><a href=#wasserstein-距离wasserstein-distance>Wasserstein 距离（Wasserstein Distance）</a></li><li><a href=#巴氏距离bhattacharyya-distance>巴氏距离（Bhattacharyya Distance）</a></li><li><a href=#最大均值差异maximum-mean-discrepancy>最大均值差异（Maximum Mean Discrepancy）</a></li><li><a href=#点间互信息pointwise-mutual-information>点间互信息（Pointwise Mutual Information）</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/%E7%BC%96%E7%A8%8B%E7%8F%A0%E7%8E%91/>编程珠玑</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/>机器学习中常用的距离度量汇总</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Aug 18, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 16 分钟</time></div></footer></div></header><section class=article-content><h2 id=距离的定义>距离的定义</h2><p>在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于<strong>距离度量函数</strong>$d(x,y)$，其需要满足如下性质：</p><ul><li><strong>非负性</strong>：$d(x,y)\geq 0$</li><li><strong>同一性</strong>：$d(x,y)=0\Leftrightarrow x=y$</li><li><strong>对称性</strong>：$d(x,y)=d(y,x)$</li><li><strong>三角不等式</strong>：$d(x,y)\leq d(x,z)+d(z,y)$</li></ul><p>根据样本特征空间的不同，我们把度量的距离分为：<strong>空间距离</strong>、<strong>字符距离</strong>、<strong>集合距离</strong>、<strong>分布距离</strong>。</p><h2 id=空间距离>空间距离</h2><h3 id=欧几里得距离euclidean-distance>欧几里得距离（Euclidean Distance）</h3><p>欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\mathcal{L}_2$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间欧几里得距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>EuclideanDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=曼哈顿距离manhattan-distance>曼哈顿距离（Manhattan Distance）</h3><p>曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\mathcal{L}_1$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的曼哈顿距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left|x_i-y_i\right|}}$$</p><p>下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。</p><figure><img src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/Manhattan-Distance.png alt="曼哈顿距离（Manhattan Distance）"><figcaption><p>曼哈顿距离（Manhattan Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间曼哈顿距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ManhattanDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=切比雪夫距离chebyschev-distance>切比雪夫距离（Chebyschev Distance）</h3><p>切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\mathcal{L}_\infty$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的切比雪夫距离定义为：</p><p>$$d=\max\left|x_i-y_i\right|$$</p><p>以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。</p><figure><img src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/Chebyschev-Distance.png alt="切比雪夫距离（Chebyschev Distance）"><figcaption><p>切比雪夫距离（Chebyschev Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间切比雪夫距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ChebyshevDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=闵可夫斯基距离minkowski-distance>闵可夫斯基距离（Minkowski Distance）</h3><p>闵可夫斯基距离并非一种新型的距离度量方式，而是一种对于多种不同距离度量的概括性表述。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的闵可夫斯基距离定义为：</p><p>$$d=\left(\sum_{i=1}^{n}{\left|x_i-y_i\right|^p}\right)^{1/p}$$</p><p>其中，$p$为闵可夫斯基距离参数。当$p=1$时，闵可夫斯基距离退化为曼哈顿距离；当$p=2$时，闵可夫斯基距离退化为欧几里得距离；当$p\rightarrow\infty$时，闵可夫斯基距离退化为切比雪夫距离，如下图所示：</p><figure><img src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/Minkowski-Distance.png alt="闵可夫斯基距离（Minkowski Distance）"><figcaption><p>闵可夫斯基距离（Minkowski Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间闵可夫斯基距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>MinkowskiDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>zipped_coordinate</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>math</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>),</span> <span class=n>p</span><span class=p>)),</span> <span class=mi>1</span> <span class=o>/</span> <span class=n>p</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=标准化欧几里得距离standardized-euclidean-distance>标准化欧几里得距离（Standardized Euclidean Distance）</h3><p>标准化欧几里得距离是将欧式空间中任意两点的分量都“标准化”到均值、方差一致的区间，记每个分量的均值为$\mu$，方差为$\sigma_i^2$，“标准化”结果为$X^*$：</p><p>$$X^*=\frac{X-\mu}{\sigma^2}$$</p><p>对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的标准化欧几里得距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left(\frac{x_i-y_i}{\sigma_i^2}\right)^2}}$$</p><p>如果将$1/\sigma^2$看做权重，则标准化欧几里得距离可以被认为是加权欧几里得距离。</p><p>在 <code>Python</code> 中用于计算任意两点间标准化欧几里得距离的代码如下（需要注意 <code>sigma ≠ 0</code>）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>StandardizedEuclideanDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>sigma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>ddof</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(((</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span> <span class=o>/</span><span class=n>sigma</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=马氏距离mahalanobis-distance>马氏距离（Mahalanobis Distance）</h3><p>马氏距离由印度统计学家<a class=link href=https://en.wikipedia.org/wiki/Prasanta_Chandra_Mahalanobis target=_blank rel=noopener>马哈拉诺比斯（P. C. Mahalanobis）</a>提出，用于表示数据的协方差距离，它可以有效地表示两个未知样本集之间的相似度。对于$n$维空间中的任意两个样本集$\textbf{X}$和$\textbf{Y}$的马氏距离定义为：</p><p>$$d=\sqrt{\left(\textbf{X}-\textbf{Y}\right)^T\Sigma^{-1}\left(\textbf{X}-\textbf{Y}\right)}$$</p><p>其中，$\Sigma$为样本集$X$和$Y$的协方差矩阵。若协方差矩阵为单位矩阵，则马氏距离退化为欧几里得距离；若协方差矩阵为对角矩阵，则马氏距离退化为标准化欧几里得距离。</p><p>需要注意的是，马氏距离的计算需要确保$\Sigma$的逆矩阵存在，否则可以直接采用欧几里得距离进行计算。此外，马氏距离不受量纲的影响，它可以排除变量之间相关性的干扰，但同时也夸大了微小变化量的作用。例如，若将两个相同的样本放入两个不同的总体中，经计算的到的马氏距离也是不同的（除非它们的$\Sigma$恰巧相同）。</p><p>在 <code>Python</code> 中用于计算任意两个样本集间马氏距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>MahalanobisDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>X_T</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>    <span class=n>sigma_inverse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>cov</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>d</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X_T</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X_T</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>            <span class=n>delta</span> <span class=o>=</span> <span class=n>X_T</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>X_T</span><span class=p>[</span><span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>d</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>delta</span><span class=p>,</span><span class=n>sigma_inverse</span><span class=p>),</span><span class=n>delta</span><span class=o>.</span><span class=n>T</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>d</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=兰氏距离lance-and-williams-distance>兰氏距离（Lance and Williams Distance）</h3><p>兰氏距离又被称为堪培拉距离（Canberra Distance），可以理解为加权曼哈顿距离。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的兰氏距离定义为：</p><p>$$d=\sum_{i=1}^{n}{\frac{\left|x_i-y_i\right|}{\left|x_i\right|+\left|y_i\right|}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间兰氏距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>CanberraDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>d</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>and</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>d</span> <span class=o>+=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>d</span> <span class=o>+=</span> <span class=nb>abs</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>/</span> <span class=p>(</span><span class=nb>abs</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>+</span> <span class=nb>abs</span><span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>d</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=余弦相似度cosine-similarity>余弦相似度（Cosine Similarity）</h3><p>在几何学中，通常采用余弦相似度来度量两个向量间的夹角，其取值为$[-1,1]$。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的余弦相似度定义为：</p><p>$$\cos{\left(\vec{x},\vec{y}\right)}=\frac{\vec{x}\cdot\vec{y}}{\left|\vec{x}\right|\cdot\left|\vec{y}\right|}=\frac{\displaystyle\sum_{i=1}^{n}{x_i\cdot y_i}}{\sqrt{\displaystyle\sum_{i=1}^{n}{x_i^2}}\cdot\sqrt{\displaystyle\sum_{i=1}^{n}{y_i^2}}}$$</p><p>当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow0$时，两向量完全正交；当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow1$时，两向量完全重合；当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow -1$时，两向量完全相反。在 <code>Python</code> 中用于计算任意两点间余弦相似度的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>CosineDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>y</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=测地距离geodesic-distance>测地距离（Geodesic Distance）</h3><p>测地距离原指球体表面上两点间的最短距离，后来被推广到其它领域。在图论中，测地距离为两顶点间的最短路径；在欧式空间中，测地距离为欧几里得距离；在非欧空间中，测地距离为连接两点间的最短圆弧。如下图所示：</p><figure><img src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/Geodesic-Distance.jpg alt="测地距离（Geodesic Distance）"><figcaption><p>测地距离（Geodesic Distance）</p></figcaption></figure><h3 id=布雷柯蒂斯距离bray-curtis-distance>布雷柯蒂斯距离（Bray Curtis Distance）</h3><p>布雷柯蒂斯距离主要用于生态学和环境科学领域，用于计算不同样本间的差异。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的布雷柯蒂斯距离定义为：</p><p>$$d=\frac{\displaystyle \sum_{i=1}^{n}{\left|x_i-y_i\right|}}{\displaystyle \sum_{i=1}^{n}{x_i}+\sum_{i=1}^{n}{y_i}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间布雷柯蒂斯距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>import numpy as np
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>def BrayCurtisDistance(x, y):
</span></span><span class=line><span class=cl>    x = np.array(x)
</span></span><span class=line><span class=cl>    y = np.array(y)
</span></span><span class=line><span class=cl>    return np.sum(np.abs(x - y)) / (np.sum(x) + np.sum(y))
</span></span></code></pre></td></tr></table></div></div><h3 id=半正矢距离haversine-distance>半正矢距离（Haversine Distance）</h3><p>半正矢距离用于计算任意两经纬点间的距离，对于空间中的任意两经纬点$A(\mathrm{lon}_1,\mathrm{lat}_1)$和$B(\mathrm{lon}_2,\mathrm{lat}_2)$的半正矢距离定义为：</p><p>$$d=2r\cdot\arcsin{\sqrt{\sin^2{\frac{\mathrm{lat}_2-\mathrm{lat}_1}{2}}+\cos{(\mathrm{lat}_1)}\cos{(\mathrm{lat}_2)}\sin^2{\frac{\mathrm{lon}_2-\mathrm{lon}_1}{2}}}}$$</p><p>其中，$r$为半径。在 <code>Python</code> 中用于计算任意两经纬点间半正矢距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>HaversineDistance</span><span class=p>(</span><span class=n>lon1</span><span class=p>,</span> <span class=n>lat1</span><span class=p>,</span> <span class=n>lon2</span><span class=p>,</span> <span class=n>lat2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>lon1</span><span class=p>,</span> <span class=n>lat1</span><span class=p>,</span> <span class=n>lon2</span><span class=p>,</span> <span class=n>lat2</span> <span class=o>=</span> <span class=nb>map</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>radians</span><span class=p>,</span> <span class=p>[</span><span class=n>lon1</span><span class=p>,</span> <span class=n>lat1</span><span class=p>,</span> <span class=n>lon2</span><span class=p>,</span> <span class=n>lat2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>c</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=mi>6367</span> <span class=o>*</span> <span class=mi>1000</span> <span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>arcsin</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>((</span><span class=n>lat2</span> <span class=o>-</span> <span class=n>lat1</span><span class=p>)</span> <span class=o>/</span> <span class=mf>2.0</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>lat1</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>lat2</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>((</span><span class=n>lon2</span> <span class=o>-</span> <span class=n>lon1</span><span class=p>)</span> <span class=o>/</span> <span class=mf>2.0</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=字符距离>字符距离</h2><h3 id=汉明距离hamming-distance>汉明距离（Hamming Distance）</h3><p>汉明距离由两个字符串对应位不同的数量决定，通常用于数据传输差错控制编码领域。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的汉明距离定义为：</p><p>$$d=\sum_{i=0}^{n}{x_i\otimes y_i}$$</p><p>汉明距离也可以理解为将$A$变为$B$的最小操作次数。在 <code>Python</code> 中用于计算任意两个字符串间汉明距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>HammingDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>x_ch</span> <span class=o>!=</span> <span class=n>y_ch</span> <span class=k>for</span> <span class=n>x_ch</span><span class=p>,</span> <span class=n>y_ch</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=莱文斯坦距离levenshtein-distance>莱文斯坦距离（Levenshtein Distance）</h3><p>莱文斯坦距离又被称为编辑距离（Edit Distance），用于度量两个字符串之间的差异，定义为：将字符串$A$转化为字符串$B$所需的最少单字符编辑（插入、删除或替换）次数。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$，$A$的前$i$个字符和$B$的前$j$个字符的莱文斯坦距离定义为：</p><p>$$d(i,j) = \begin{align*} \left{\begin{matrix} \max{\left(i,j\right)}, & \min{\left(i,j\right)}=0 \ \min{\left[d(i-1,j),d(i,j-1), d(i,j)\right]} + I(i,j), & \min{\left(i,j\right)\neq 0} \end{matrix}\right. \end{align*}$$</p><p>其中，$I(\cdot)$为指示函数，当$x_i=y_j$时，$I(i,j)=0$；当$x_i\neq y_j$时，$I(i,j)=1$。在 <code>Python</code> 中用于计算任意两个字符串间莱文斯坦距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>LevenshteinDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>dp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>i</span>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>dp</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>j</span>
</span></span><span class=line><span class=cl>     
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>delta</span> <span class=o>=</span> <span class=mi>0</span> <span class=k>if</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=n>y</span><span class=p>[</span><span class=n>j</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=k>else</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>dp</span><span class=p>[</span><span class=n>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>][</span><span class=n>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>delta</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>int</span><span class=p>(</span><span class=n>dp</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)][</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)])</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=归一化-google-距离normalized-google-distance>归一化 Google 距离（Normalized Google Distance）</h3><p>归一化 Google 距离是由给定一组关键词集合的 Google 搜索引擎所返回的命中数量决定的，它是一种语义相似度量方法。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的归一化 Google 距离定义为：</p><p>$$d=\frac{\max{\left[\log{f(A)},\log{f(B)}\right]}-\log{f(A,B)}}{\log{M}-\min{\left[\log{f(A)}, \log{f(B)}\right]}}$$</p><p>其中，$M$为 Google 搜索引擎所返回的网页总数；$f(x)$和$f(y)$分别为 Google 搜索引擎返回关于字符串$A$和字符串$B$的命中数量；$f(A,B)$为 Google 搜索引擎返回关于字符串$A$和字符串$B$同时出现的命中数量。</p><h3 id=jaro-winkler-相似度jaro-winkler-similarity>Jaro-Winkler 相似度（<strong>J</strong>aro-Winkler Sim<strong>ilarity</strong>）</h3><p>Jaro 相似度用于评估两个字符串间的相似度，对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的 Jaro 相似度定义为：</p><p>$$d_j=\frac{1}{3}\cdot\left(\frac{m}{|A|}+\frac{m}{|B|}+\frac{m-t}{m}\right)$$</p><p>其中，$m$是匹配的字符数，$t$是替换的字符数。</p><p>Jaro-Winkler 相似度在 Jaro 相似度的基础上引入了前缀的概念，对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的 Jaro-Winkler 相似度定义为：</p><p>$$d_w=d_j+l\cdot p\cdot(1-d_j)$$</p><p>其中，$l$为字符串$A$和字符串$B$的共同前缀的字符数；$p$为缩放因子（常取$p=0.1$）。</p><h3 id=李距离lee-distance>李距离（Lee Distance）</h3><p>李距离是编码理论中用于描述字符串距离的方案，对于使用包含$q$个字母的字母表且长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的李距离定义为：</p><p>$$d=\sum_{i=0}^{n}{\min{\left(\left|x_i-y_i\right|,q-\left|x_i-y_i\right|\right)}}$$</p><p>当$q=2$或$q=3$时，李距离退化为汉明距离。</p><h2 id=集合距离>集合距离</h2><h3 id=杰卡德相似系数jaccard-similarity-coefficient>杰卡德相似系数（Jaccard Similarity Coefficient）</h3><p>杰卡德相似系数是指两个集合$A$和$B$中相同元素在所有元素中的占比，定义为$J(A,B)$：</p><p>$$J(A,B)=\frac{\left|A\cap B\right|}{\left|A\cup B\right|}$$</p><p>杰卡德距离（Jaccard Distance）是指两个集合$A$和$B$中不同元素在所有元素中的占比，定义为$J_\delta(A,B)$：</p><p>$$J_\delta(A,B)=1-J(A,B)=1-\frac{\left|A\cap B\right|}{\left|A\cup B\right|}$$</p><p>在 <code>Python</code> 中用于计算任意两个集合间杰卡德相似系数的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>JaccardSimilarityCoefficient</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>double</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>bitwise_and</span><span class=p>((</span><span class=n>x</span> <span class=o>!=</span> <span class=n>y</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>bitwise_or</span><span class=p>(</span><span class=n>x</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>y</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>double</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>bitwise_or</span><span class=p>(</span><span class=n>x</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>y</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=奥奇亚系数ochiia-coefficient>奥奇亚系数（Ochiia Coefficient）</h3><p>奥奇亚系数是指两个集合$A$和$B$中相同元素与两集合大小几何平均值的比值，定义为$K(A,B)$：</p><p>$$K(A,B)=\frac{\left|A\cap B\right|}{\sqrt{\left|A\right|\times\left|B\right|}}$$</p><h3 id=戴斯系数dice-coefficient>戴斯系数（Dice Coefficient）</h3><p>戴斯系数除了可以用来衡量两个集合间的距离，还可以用来衡量两个字符串间的距离，定义为$D(A,B)$：</p><p>$$D(A,B)=\frac{2\left|A\cap B\right|}{\left|A\right|+\left|B\right|}$$</p><h3 id=豪斯多夫距离hausdorff-distance>豪斯多夫距离（Hausdorff Distance）</h3><p>豪斯多夫距离用于度量两个集合$A$和$B$间的距离，定义为$H(A,B)$：</p><p>$$H(A,B)=\max{\left[h(A,B),h(B,A)\right]}$$</p><p>其中，$h(\cdot)$为双向豪斯多夫距离，定义为：</p><p>$$h(A,B)=\max_{a\in A}{\min_{b\in B}{\left |a-b\right|}}$$</p><p>其中，$h(A,B)$为从集合$A$到集合$B$的单向豪斯多夫距离，$h(B,A)$为从集合$B$到集合$A$的单向豪斯多夫距离。</p><h2 id=分布距离>分布距离</h2><h3 id=皮尔逊相关系数pearson-correlation-coefficient>皮尔逊相关系数（Pearson Correlation Coefficient）</h3><p>皮尔逊相关系数又被称为皮尔逊积矩相关技术（Pearson Product Moment Correlation Coefficient，PPMCC/PCC），用于度量两个变量$X$和$Y$之间的线性相关性。与上文提到的余弦相似度不同，皮尔逊相关系数不受平移变换的影响。对于$n$维空间中的任意两个分布$X$和$Y$的皮尔逊相关系数定义为：</p><p>$$\rho(X,Y)=\frac{\mathrm{cov}(X,Y)}{\sigma(X)\cdot\sigma(Y)}=\frac{\mathrm{E}\left[\left(X-X_\mu\right)\left(Y-Y_\mu\right)\right]}{\sigma(X)\cdot\sigma(Y)}$$</p><p>皮尔逊相关系数与余弦相似度的关系为：</p><p>$$\rho(X,Y)=\frac{\mathrm{E}\left[\left(X-X_\mu\right)\left(Y-Y_\mu\right)\right]}{\sigma(X)\cdot\sigma(Y)}=\frac{\displaystyle\sum_{i=1}^{n}{\left(X-X_\mu\right)\cdot\left(Y-Y_\mu\right)}}{\left|X-X_\mu\right|\cdot\left|Y-Y_\mu\right|}=\cos{\left(X-X_\mu,Y-Y_\mu\right)}$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间皮尔逊相关系数的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>PearsonCorrelationCoefficient</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y_</span> <span class=o>=</span> <span class=n>y</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_</span><span class=p>,</span> <span class=n>y_</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x_</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>y_</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=卡方度量chi-square-measure>卡方度量（Chi-square Measure）</h3><p>$\mathcal{X}^2$检验通常用于检验某一观测分布是否符合典型理论分布。若观测频数与期望频数差异越小，则$\mathcal{X}^2$值越小；若观测频数与期望频数差异越大，则$\mathcal{X}^2$值越大。因此，$\mathcal{X}^2$值可以用于描述观测分布与理论分布的差异。对于$n$维空间中的任意两个分布$X$和$Y$的$\mathcal{X}^2$统计量定义为：</p><p>$$\mathcal{X}^2=\sum_{i=1}^{n}{\frac{\left(x_i-y_i\right)^2}{y_i}}=\sum_{i=1}^{k}{\frac{\left(x_i-n\cdot p_i\right)^2}{k\cdot p_i}}$$</p><p>其中，$x_i$为$X$在$i$的频数，$y_i$为$Y$在$i$的频数，$k$为总频数，$p_i$为$Y$在$i$的概率。在 <code>Python</code> 中用于计算任意两个分布间卡方度量的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ChiSquareMeasure</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>)</span> <span class=o>/</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=交叉熵cross-entropy>交叉熵（Cross Entropy）</h3><p>交叉熵是香农信息论中的重要概念，用于度量两个分布之间的差异信息。对于$n$维空间中的任意两个分布$X$和$Y$的交叉熵定义为：</p><p>$$H(X,Y)=-\int_{p}{X(p)\cdot Y(p)\mathrm{d}p}$$</p><p>若基于分布$X$对分布$X$进行编码，其编码长度的期望为：</p><p>$$H(X)=-\int_{p}{X(p)\mathrm{d}p}$$</p><p>若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$。在 <code>Python</code> 中用于计算任意两个分布间交叉熵的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>CrossEntropy</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>x</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>y</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=kl-散度kullback-leibler-divergence>KL 散度（Kullback-Leibler Divergence）</h3><p>KL 散度又被称为相对熵（Relative Entropy）或信息散度（Information Divergence），用于度量两个分布间的差异，对于$n$维空间中的任意两个分布$X$和$Y$的 KL 散度定义为：</p><p>$$\mathrm{KL}(X|Y)=\int_{p}{X(p)\cdot\log{\frac{X(p)}{Y(p)}}\mathrm{d}p}$$</p><p>若基于分布$X$对分布$X$进行编码，其编码长度的期望为：</p><p>$$H(X)=-\int_{p}{X(p)\log{X(p)}\mathrm{d}p}$$</p><p>若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$，其多出的编码长度为：</p><p>$$\mathrm{KL}(X|Y)=H(X)-H(X,Y)$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间 KL 散度的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>KullbackLeiblerDivergence</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>p</span> <span class=o>/</span> <span class=n>q</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=js-散度jensen-shannon-divergence>JS 散度（Jensen-Shannon Divergence）</h3><p>JS 散度是 KL 散度的变体，解决了 KL 散度非对称的问题，对于$n$维空间中的任意两个分布$X$和$Y$的 JS 散度定义为：</p><p>$$\mathrm{JS}(X|Y)=\frac{1}{2}\cdot\mathrm{KL}(X|\frac{X+Y}{2})+\frac{1}{2}\cdot\mathrm{KL}(Y|\frac{X+Y}{2})$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间 JS 散度的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>JensenShannonDivergence</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>p</span><span class=o>/</span><span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>q</span><span class=p>)))</span> <span class=o>+</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>q</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>q</span><span class=o>/</span><span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>q</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=海林格距离hellinger-distance>海林格距离（Hellinger Distance）</h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的海林格距离定义为：</p><p>$$d={\frac{1}{\sqrt{2}}\cdot\sqrt{\sum_{i=1}^{n}{\left(\sqrt{x_i}-\sqrt{y_i}\right)^2}}}$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间海林格距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>HellingerDistance</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>q</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=α-散度α-divergence>α 散度（α Divergence）</h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的 α 散度被定义为：</p><p>$$d=\frac{4}{1-\alpha^2}\cdot\left[1-\int_p{X(p)^{(1+\alpha)/2}\cdot Y(p)^{(1-\alpha)/2}}\mathrm{d}x\right]$$</p><p>其中，$-\infty&lt;\alpha&lt;+\infty$为连续参数。当$\alpha\rightarrow 1$时，α 散度退化为 KL 散度；当$\alpha\rightarrow 0$时，α 散度退化为海林格距离（仅相差常系数）。</p><h3 id=f-散度f-divergence>F 散度（F Divergence）</h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的 F 散度被定义为：</p><p>$$\mathrm{D}(X|Y)=\int_p{Y(p)\cdot f\left[\frac{X(p)}{Y(p)}\right]\mathrm{d}p}$$</p><p>其中，函数$f(\cdot)$需满足：（1）$f(\cdot)$为凸函数；（2）$f(1)=0$。下表给出了$f(\cdot)$取不同值时，F 散度对应的结果。</p><div class=table-wrapper><table><thead><tr><th><strong>散度</strong></th><th>$f(\cdot)$</th></tr></thead><tbody><tr><td>卡方距离</td><td>$(t-1)^2$</td></tr><tr><td>KL 散度</td><td>$x\log{x}$</td></tr><tr><td>逆 KL 散度</td><td>$-\log{x}$</td></tr><tr><td>海林格距离</td><td>$\left(\sqrt{x}-1\right)^2$</td></tr><tr><td>α 散度</td><td>$4/(1-\alpha)^2\cdot\left(1-x^{(1+\alpha)/2}\right)$</td></tr></tbody></table></div><p>在 <code>Python</code> 中用于计算任意两个分布间 F 散度（海林格距离）的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>t</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>FDivergence</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>q</span> <span class=o>*</span> <span class=n>f</span><span class=p>(</span><span class=n>p</span> <span class=o>/</span> <span class=n>q</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=布雷格曼散度bregman-divergence>布雷格曼散度（Bregman Divergence）</h3><p>对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：</p><p>$$d(x,y)=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$</p><p>将上式两侧平方后，得到：</p><p>$$d^2(x,y)=\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}$$</p><p>定义$&lt;x,y>=\sum_{i=1}^{n}{x_i\cdot y_i}$，$\left|x\right|=\sqrt{&lt;x,x>}$，上式可改写为：</p><p>$$d^2(x,y)=\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}=&lt;x-y,x-y>=\left|x\right|^2-\left(\left|y\right|^2+&lt;2y,x-y>\right)$$</p><p>此处的距离即为欧几里得模函数和其在$x$处切线在$y$处的点估计差。推广该概念后，对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的布雷格曼散度定义为：</p><p>$$d(x,y)=f(x)-\left[f(y)+&lt;\nabla f(x),x-y >\right]$$</p><p>表给出了$f(\cdot)$取不同值时，布雷格曼散度对应的结果。</p><div class=table-wrapper><table><thead><tr><th><strong>散度</strong></th><th>$f(\cdot)$</th></tr></thead><tbody><tr><td>平方损失</td><td>$x^2$</td></tr><tr><td>/</td><td>$x\log{x}$</td></tr><tr><td>Logistic 损失</td><td>$x\log{x}+(1-x)\log{(1-x)}$</td></tr><tr><td>Itakura-Saito 距离</td><td>$-\log{x}$</td></tr><tr><td>/</td><td>$e^x$</td></tr><tr><td>平方欧几里得距离</td><td>$\left|x\right|^2$</td></tr><tr><td>马氏距离</td><td>$\mathbf{X}^TA\mathbf{X}$</td></tr><tr><td>KL 散度</td><td>$\sum_{i=1}^{n}{x_i\log{x_i}}$</td></tr></tbody></table></div><h3 id=wasserstein-距离wasserstein-distance>Wasserstein 距离（Wasserstein Distance）</h3><p>Wasserstein 距离被称为推土机距离，用于表示两个分布的相似度。Wasserstein 距离定义为把分布$X$转变成分布$Y$所需移动的平均距离的最小值，如下图所示：</p><figure><img src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/Wasserstein-Distance.png alt="Wasserstein 距离（Wasserstein Distance）"><figcaption><p>Wasserstein 距离（Wasserstein Distance）</p></figcaption></figure><p>对于$n$维空间中的任意两个分布$X$和$Y$的 Wasserstein 距离定义为：</p><p>$$d=\inf_{\gamma\sim\Pi(X,Y)}{\mathrm{E}_{p,q\sim\gamma}\left|p-q\right|}$$</p><p>其中，$\Pi(X,Y)$为分布$X$和分布$Y$构成的联合分布的集合，$\gamma$为$\Pi(X,Y)$中任意分布，$p$和$q$是分布$\gamma$中的样本。</p><h3 id=巴氏距离bhattacharyya-distance>巴氏距离（Bhattacharyya Distance）</h3><p>巴氏系数可以用来度量两个分布的相似性，对于$n$维空间中的任意两个分布$X$和$Y$的巴氏系数定义为：</p><p>$$c_b=\int_{p}{\sqrt{X(p)\cdot Y(p)}\mathrm{d}p}$$</p><p>巴氏距离定义为：</p><p>$$d_b=-\ln{c_b}$$</p><p>需要注意的是，海林格距离$d=\sqrt{1-d_b}$。在 <code>Python</code> 中用于计算任意两个分布间巴氏距离的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>BhattacharyyaCoefficient</span><span class=p>(</span><span class=n>p</span><span class=p>,</span><span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=n>q</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>HellingerDistance</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>BhattacharyyaCoefficient</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>BhattacharyyaDistance</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>BhattacharyyaCoefficient</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=最大均值差异maximum-mean-discrepancy>最大均值差异（Maximum Mean Discrepancy）</h3><p>最大均值差异是迁移学习领域中最为广泛使用的一种损失函数，它度量了再生希尔伯特空间中两个不同分布间的距离。通过在样本空间寻找连续函数$f:X\rightarrow R$随机投影后，分别求这两个分布在$f$上函数值的均值，并通过做差得到均值差异（Mean Discrepancy）。最大化均值差异即为寻找一个$f$使得均值差异最大。对于$n$维空间中的任意两个分布$X$和$Y$的最大均值差异定义为：</p><p>$$d=\sup_{|f|_\mathrm{H} \leq 1}{\mathrm{E}_p[f(X)]-\mathrm{E}_q[f(Y)]}$$</p><h3 id=点间互信息pointwise-mutual-information>点间互信息（Pointwise Mutual Information）</h3><p>点间互信息用来衡量两个分布的相关性，对于$n$维空间中的任意两个分布$X$和$Y$的点间互信息定义为：</p><p>$$d=\log{\frac{p(X,Y)}{p(X)\cdot p(Y)}}=\log{\frac{p(X|Y)}{p(X)}}=\log{\frac{p(Y|X)}{p(Y)}}$$</p><p>若$X$和$Y$不相关，则$P(X,Y)=P(X)\cdot P(Y)$。</p></section><footer class=article-footer><section class=article-tags><a href=/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a>
<a href=/tags/%E6%B5%8B%E5%BA%A6%E8%AE%BA/>测度论</a>
<a href=/tags/%E7%A9%BA%E9%97%B4%E8%B7%9D%E7%A6%BB/>空间距离</a>
<a href=/tags/%E5%AD%97%E7%AC%A6%E8%B7%9D%E7%A6%BB/>字符距离</a>
<a href=/tags/%E9%9B%86%E5%90%88%E8%B7%9D%E7%A6%BB/>集合距离</a>
<a href=/tags/%E5%88%86%E5%B8%83%E8%B7%9D%E7%A6%BB/>分布距离</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/sgu-144-meeting/><div class=article-details><h2 class=article-title>SGU 144 - Meeting</h2></div></a></article><article><a href=/p/sgu-116-index-of-super-prime/><div class=article-details><h2 class=article-title>SGU 116 - Index of super-prime</h2></div></a></article><article><a href=/p/%E4%B8%93%E9%A2%98%E4%B8%80%E7%AE%80%E5%8D%95%E6%90%9C%E7%B4%A2-virtual-judge/><div class=article-details><h2 class=article-title>专题一、简单搜索 - Virtual Judge</h2></div></a></article><article><a href=/p/2048-%E6%B8%B8%E6%88%8F%E5%88%B6%E4%BD%9C%E8%BF%87%E7%A8%8Bjava-%E6%8F%8F%E8%BF%B0%E7%AC%AC%E4%BA%94%E8%8A%82%E7%95%8C%E9%9D%A2%E7%BE%8E%E5%8C%96/><div class=article-details><h2 class=article-title>2048 游戏制作过程（Java 描述）：第五节、界面美化</h2></div></a></article><article><a href=/p/2048-%E6%B8%B8%E6%88%8F%E5%88%B6%E4%BD%9C%E8%BF%87%E7%A8%8Bjava-%E6%8F%8F%E8%BF%B0%E7%AC%AC%E5%9B%9B%E8%8A%82%E6%B8%B8%E6%88%8F%E9%80%BB%E8%BE%91/><div class=article-details><h2 class=article-title>2048 游戏制作过程（Java 描述）：第四节、游戏逻辑</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kwang-2.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2012 -
2024 退思轩</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>
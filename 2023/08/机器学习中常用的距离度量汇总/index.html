<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>机器学习中常用的距离度量汇总 | 退思轩</title><meta name=keywords content="神经网络,测度论,空间距离,字符距离,集合距离,分布距离"><meta name=description content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质： 非负性：$d(x,y)\geq 0$ 同一性：$d(x,y)=0\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。 空间距离 欧几里得距离（Euc"><meta name=author content="Kai Wang"><link rel=canonical href=https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/><meta name=google-site-verification content="G-FVKQ0YJ3T3"><link crossorigin=anonymous href=/assets/css/stylesheet.8a617db1a020453e788b433cfeb06be606888c4ad84bfea58b1ee6d9544a1922.css integrity="sha256-imF9saAgRT54i0M8/rBr5gaIjErYS/6lix7m2VRKGSI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kwang.life/images/others/site-favicon.png><link rel=icon type=image/png sizes=16x16 href=https://kwang.life/images/others/site-favicon.png><link rel=icon type=image/png sizes=32x32 href=https://kwang.life/images/others/site-favicon.png><link rel=apple-touch-icon href=https://kwang.life/images/others/site-favicon.png><link rel=mask-icon href=https://kwang.life/images/others/site-favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://code.jquery.com/jquery-3.3.1.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}],throwOnError:!1})})</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script src=https://sdk.jinrishici.com/v2/browser/jinrishici.js></script>
<script type=text/javascript>jinrishici.load(function(e){const t="<p style='font-size: 22px;'>"+e.data.content+"</p>"+"<p style='font-size: 16px; text-align: right;'>"+e.data.origin.dynasty+"&nbsp;·&nbsp;"+e.data.origin.author+"《"+e.data.origin.title+"》</p>";document.getElementsByClassName('first-entry')[0].childNodes[1].innerHTML=t})</script><link href=https://kwang.life/css/jquery.fancybox.min.css rel=stylesheet><script src=https://kwang.life/js/jquery.fancybox.min.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FVKQ0YJ3T3"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-FVKQ0YJ3T3',{anonymize_ip:!1})}</script><meta property="og:title" content="机器学习中常用的距离度量汇总"><meta property="og:description" content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质： 非负性：$d(x,y)\geq 0$ 同一性：$d(x,y)=0\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。 空间距离 欧几里得距离（Euc"><meta property="og:type" content="article"><meta property="og:url" content="https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-18T11:27:00+08:00"><meta property="article:modified_time" content="2023-08-18T11:27:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="机器学习中常用的距离度量汇总"><meta name=twitter:description content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质： 非负性：$d(x,y)\geq 0$ 同一性：$d(x,y)=0\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。 空间距离 欧几里得距离（Euc"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://kwang.life/posts/"},{"@type":"ListItem","position":3,"name":"机器学习中常用的距离度量汇总","item":"https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"机器学习中常用的距离度量汇总","name":"机器学习中常用的距离度量汇总","description":"距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质： 非负性：$d(x,y)\\geq 0$ 同一性：$d(x,y)=0\\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。 空间距离 欧几里得距离（Euc","keywords":["神经网络","测度论","空间距离","字符距离","集合距离","分布距离"],"articleBody":"距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质：\n非负性：$d(x,y)\\geq 0$ 同一性：$d(x,y)=0\\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。\n空间距离 欧几里得距离（Euclidean Distance） 欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\\mathcal{L}_2$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的欧几里得距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}}$$\n在 Python 中用于计算任意两点间欧几里得距离的代码如下：\nimport numpy as np def EuclideanDistance(x, y): x = np.array(x) y = np.array(y) return np.sqrt(np.sum(np.square(x - y))) 曼哈顿距离（Manhattan Distance） 曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\\mathcal{L}_1$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的曼哈顿距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left|x_i-y_i\\right|}}$$\n下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。\n图 1：曼哈顿距离（Manhattan Distance）\n在 Python 中用于计算任意两点间曼哈顿距离的代码如下：\nimport numpy as np def ManhattanDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) 切比雪夫距离（Chebyschev Distance） 切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\\mathcal{L}_\\infty$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的切比雪夫距离定义为：\n$$d=\\max\\left|x_i-y_i\\right|$$\n以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。\n图 2：切比雪夫距离（Chebyschev Distance）\n在 Python 中用于计算任意两点间切比雪夫距离的代码如下：\nimport numpy as np def ChebyshevDistance(x, y): x = np.array(x) y = np.array(y) return np.max(np.abs(x - y)) 闵可夫斯基距离（Minkowski Distance） 闵可夫斯基距离并非一种新型的距离度量方式，而是一种对于多种不同距离度量的概括性表述。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的闵可夫斯基距离定义为：\n$$d=\\left(\\sum_{i=1}^{n}{\\left|x_i-y_i\\right|^p}\\right)^{1/p}$$\n其中，$p$为闵可夫斯基距离参数。当$p=1$时，闵可夫斯基距离退化为曼哈顿距离；当$p=2$时，闵可夫斯基距离退化为欧几里得距离；当$p\\rightarrow\\infty$时，闵可夫斯基距离退化为切比雪夫距离，如下图所示：\n图 3：闵可夫斯基距离（Minkowski Distance）\n在 Python 中用于计算任意两点间闵可夫斯基距离的代码如下：\nimport math import numpy as np def MinkowskiDistance(x, y, p): zipped_coordinate = zip(x, y) return math.pow(np.sum(math.pow(np.abs(x - y), p)), 1 / p) 标准化欧几里得距离（Standardized Euclidean Distance） 标准化欧几里得距离是将欧式空间中任意两点的分量都“标准化”到均值、方差一致的区间，记每个分量的均值为$\\mu$，方差为$\\sigma_i^2$，“标准化”结果为$X^*$：\n$$X^*=\\frac{X-\\mu}{\\sigma^2}$$\n对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的标准化欧几里得距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left(\\frac{x_i-y_i}{\\sigma_i^2}\\right)^2}}$$\n如果将$1/\\sigma^2$看做权重，则标准化欧几里得距离可以被认为是加权欧几里得距离。\n在 Python 中用于计算任意两点间标准化欧几里得距离的代码如下（需要注意 sigma ≠ 0）：\nimport numpy as np def StandardizedEuclideanDistance(x, y): x = np.array(x) y = np.array(y) X = np.vstack([x,y]) sigma = np.var(X, axis=0, ddof=1) return np.sqrt(((x - y) ** 2 /sigma).sum()) 马氏距离（Mahalanobis Distance） 马氏距离由印度统计学家马哈拉诺比斯（P. C. Mahalanobis）提出，用于表示数据的协方差距离，它可以有效地表示两个未知样本集之间的相似度。对于$n$维空间中的任意两个样本集$\\textbf{X}$和$\\textbf{Y}$的马氏距离定义为：\n$$d=\\sqrt{\\left(\\textbf{X}-\\textbf{Y}\\right)^T\\Sigma^{-1}\\left(\\textbf{X}-\\textbf{Y}\\right)}$$\n其中，$\\Sigma$为样本集$X$和$Y$的协方差矩阵。若协方差矩阵为单位矩阵，则马氏距离退化为欧几里得距离；若协方差矩阵为对角矩阵，则马氏距离退化为标准化欧几里得距离。\n需要注意的是，马氏距离的计算需要确保$\\Sigma$的逆矩阵存在，否则可以直接采用欧几里得距离进行计算。此外，马氏距离不受量纲的影响，它可以排除变量之间相关性的干扰，但同时也夸大了微小变化量的作用。例如，若将两个相同的样本放入两个不同的总体中，经计算的到的马氏距离也是不同的（除非它们的$\\Sigma$恰巧相同）。\n在 Python 中用于计算任意两个样本集间马氏距离的代码如下：\nimport numpy as np def MahalanobisDistance(x, y): x = np.array(x) y = np.array(y) X = np.vstack([x,y]) X_T = X.T sigma_inverse = np.linalg.inv(np.cov(X)) d = [] for i in range(0, X_T.shape[0]): for j in range(i + 1, X_T.shape[0]): delta = X_T[i] - X_T[j] d.append(np.sqrt(np.dot(np.dot(delta,sigma_inverse),delta.T))) return d 兰氏距离（Lance and Williams Distance） 兰氏距离又被称为堪培拉距离（Canberra Distance），可以理解为加权曼哈顿距离。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的兰氏距离定义为：\n$$d=\\sum_{i=1}^{n}{\\frac{\\left|x_i-y_i\\right|}{\\left|x_i\\right|+\\left|y_i\\right|}}$$\n在 Python 中用于计算任意两点间兰氏距离的代码如下：\nimport numpy as np def CanberraDistance(x, y): x = np.array(x) y = np.array(y) d = 0 for i in range(len(x)): if x[i] == 0 and y[i] == 0: d += 0 else: d += abs(x[i] - y[i]) / (abs(x[i]) + abs(y[i])) return d 余弦相似度（Cosine Similarity） 在几何学中，通常采用余弦相似度来度量两个向量间的夹角，其取值为$[-1,1]$。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的余弦相似度定义为：\n$$\\cos{\\left(\\vec{x},\\vec{y}\\right)}=\\frac{\\vec{x}\\cdot\\vec{y}}{\\left|\\vec{x}\\right|\\cdot\\left|\\vec{y}\\right|}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i\\cdot y_i}}{\\sqrt{\\displaystyle\\sum_{i=1}^{n}{x_i^2}}\\cdot\\sqrt{\\displaystyle\\sum_{i=1}^{n}{y_i^2}}}$$\n当$\\cos{\\left(\\vec{x},\\vec{y}\\right)}\\rightarrow0$时，两向量完全正交；当$\\cos{\\left(\\vec{x},\\vec{y}\\right)}\\rightarrow1$时，两向量完全重合；当$\\cos{\\left(\\vec{x},\\vec{y}\\right)}\\rightarrow -1$时，两向量完全相反。在 Python 中用于计算任意两点间余弦相似度的代码如下：\nimport numpy as np def CosineDistance(x, y): x = np.array(x) y = np.array(y) return np.dot(x,y) / (np.linalg.norm(x) * np.linalg.norm(y)) 测地距离（Geodesic Distance） 测地距离原指球体表面上两点间的最短距离，后来被推广到其它领域。在图论中，测地距离为两顶点间的最短路径；在欧式空间中，测地距离为欧几里得距离；在非欧空间中，测地距离为连接两点间的最短圆弧。如下图所示：\n图 4：测地距离（Geodesic Distance）\n布雷柯蒂斯距离（Bray Curtis Distance） 布雷柯蒂斯距离主要用于生态学和环境科学领域，用于计算不同样本间的差异。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的布雷柯蒂斯距离定义为：\n$$d=\\frac{\\displaystyle \\sum_{i=1}^{n}{\\left|x_i-y_i\\right|}}{\\displaystyle \\sum_{i=1}^{n}{x_i}+\\sum_{i=1}^{n}{y_i}}$$\n在 Python 中用于计算任意两点间布雷柯蒂斯距离的代码如下：\nimport numpy as np def BrayCurtisDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) / (np.sum(x) + np.sum(y)) 半正矢距离（Haversine Distance） 半正矢距离用于计算任意两经纬点间的距离，对于空间中的任意两经纬点$A(\\mathrm{lon}_1,\\mathrm{lat}_1)$和$B(\\mathrm{lon}_2,\\mathrm{lat}_2)$的半正矢距离定义为：\n$$d=2r\\cdot\\arcsin{\\sqrt{\\sin^2{\\frac{\\mathrm{lat}_2-\\mathrm{lat}_1}{2}}+\\cos{(\\mathrm{lat}_1)}\\cos{(\\mathrm{lat}_2)}\\sin^2{\\frac{\\mathrm{lon}_2-\\mathrm{lon}_1}{2}}}}$$\n其中，$r$为半径。在 Python 中用于计算任意两经纬点间半正矢距离的代码如下：\nimport numpy as np def HaversineDistance(lon1, lat1, lon2, lat2): lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) c = 2 * 6367 * 1000 *np.arcsin(np.sqrt(np.sin((lat2 - lat1) / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin((lon2 - lon1) / 2.0) ** 2)) 字符距离 汉明距离（Hamming Distance） 汉明距离由两个字符串对应位不同的数量决定，通常用于数据传输差错控制编码领域。对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的汉明距离定义为：\n$$d=\\sum_{i=0}^{n}{x_i\\otimes y_i}$$\n汉明距离也可以理解为将$A$变为$B$的最小操作次数。在 Python 中用于计算任意两个字符串间汉明距离的代码如下：\ndef HammingDistance(x, y): return sum(x_ch != y_ch for x_ch, y_ch in zip(x, y)) 莱文斯坦距离（Levenshtein Distance） 莱文斯坦距离又被称为编辑距离（Edit Distance），用于度量两个字符串之间的差异，定义为：将字符串$A$转化为字符串$B$所需的最少单字符编辑（插入、删除或替换）次数。对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$，$A$的前$i$个字符和$B$的前$j$个字符的莱文斯坦距离定义为：\n$$d(i,j) = \\begin{align*} \\left{\\begin{matrix} \\max{\\left(i,j\\right)}, \u0026 \\min{\\left(i,j\\right)}=0 \\ \\min{\\left[d(i-1,j),d(i,j-1), d(i,j)\\right]} + I(i,j), \u0026 \\min{\\left(i,j\\right)\\neq 0} \\end{matrix}\\right. \\end{align*}$$\n其中，$I(\\cdot)$为指示函数，当$x_i=y_j$时，$I(i,j)=0$；当$x_i\\neq y_j$时，$I(i,j)=1$。在 Python 中用于计算任意两个字符串间莱文斯坦距离的代码如下：\ndef LevenshteinDistance(x, y): dp = np.zeros((len(x) + 1,len(y) + 1)) for i in range(len(x) + 1): dp[i][0] = i for j in range(len(y) + 1): dp[0][j] = j for i in range(1, len(x) + 1): for j in range(1, len(y) + 1): delta = 0 if x[i-1] == y[j-1] else 1 dp[i][j] = min(dp[i - 1][j - 1] + delta, min(dp[i-1][j] + 1, dp[i][j - 1] + 1)) return int(dp[len(x)][len(y)]) 归一化 Google 距离（Normalized Google Distance） 归一化 Google 距离是由给定一组关键词集合的 Google 搜索引擎所返回的命中数量决定的，它是一种语义相似度量方法。对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的归一化 Google 距离定义为：\n$$d=\\frac{\\max{\\left[\\log{f(A)},\\log{f(B)}\\right]}-\\log{f(A,B)}}{\\log{M}-\\min{\\left[\\log{f(A)}, \\log{f(B)}\\right]}}$$\n其中，$M$为 Google 搜索引擎所返回的网页总数；$f(x)$和$f(y)$分别为 Google 搜索引擎返回关于字符串$A$和字符串$B$的命中数量；$f(A,B)$为 Google 搜索引擎返回关于字符串$A$和字符串$B$同时出现的命中数量。\nJaro-Winkler 相似度（Jaro-Winkler Similarity） Jaro 相似度用于评估两个字符串间的相似度，对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的 Jaro 相似度定义为：\n$$d_j=\\frac{1}{3}\\cdot\\left(\\frac{m}{|A|}+\\frac{m}{|B|}+\\frac{m-t}{m}\\right)$$\n其中，$m$是匹配的字符数，$t$是替换的字符数。\nJaro-Winkler 相似度在 Jaro 相似度的基础上引入了前缀的概念，对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的 Jaro-Winkler 相似度定义为：\n$$d_w=d_j+l\\cdot p\\cdot(1-d_j)$$\n其中，$l$为字符串$A$和字符串$B$的共同前缀的字符数；$p$为缩放因子（常取$p=0.1$）。\n李距离（Lee Distance） 李距离是编码理论中用于描述字符串距离的方案，对于使用包含$q$个字母的字母表且长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的李距离定义为：\n$$d=\\sum_{i=0}^{n}{\\min{\\left(\\left|x_i-y_i\\right|,q-\\left|x_i-y_i\\right|\\right)}}$$\n当$q=2$或$q=3$时，李距离退化为汉明距离。\n集合距离 杰卡德相似系数（Jaccard Similarity Coefficient） 杰卡德相似系数是指两个集合$A$和$B$中相同元素在所有元素中的占比，定义为$J(A,B)$：\n$$J(A,B)=\\frac{\\left|A\\cap B\\right|}{\\left|A\\cup B\\right|}$$\n杰卡德距离（Jaccard Distance）是指两个集合$A$和$B$中不同元素在所有元素中的占比，定义为$J_\\delta(A,B)$：\n$$J_\\delta(A,B)=1-J(A,B)=1-\\frac{\\left|A\\cap B\\right|}{\\left|A\\cup B\\right|}$$\n在 Python 中用于计算任意两个集合间杰卡德相似系数的代码如下：\nimport numpy as np def JaccardSimilarityCoefficient(x, y): x = np.asarray(x, np.int32) y = np.asarray(y, np.int32) return np.double(np.bitwise_and((x != y), np.bitwise_or(x != 0, y != 0)).sum()) / np.double(np.bitwise_or(x != 0, y != 0).sum()) 奥奇亚系数（Ochiia Coefficient） 奥奇亚系数是指两个集合$A$和$B$中相同元素与两集合大小几何平均值的比值，定义为$K(A,B)$：\n$$K(A,B)=\\frac{\\left|A\\cap B\\right|}{\\sqrt{\\left|A\\right|\\times\\left|B\\right|}}$$\n戴斯系数（Dice Coefficient） 戴斯系数除了可以用来衡量两个集合间的距离，还可以用来衡量两个字符串间的距离，定义为$D(A,B)$：\n$$D(A,B)=\\frac{2\\left|A\\cap B\\right|}{\\left|A\\right|+\\left|B\\right|}$$\n豪斯多夫距离（Hausdorff Distance） 豪斯多夫距离用于度量两个集合$A$和$B$间的距离，定义为$H(A,B)$：\n$$H(A,B)=\\max{\\left[h(A,B),h(B,A)\\right]}$$\n其中，$h(\\cdot)$为双向豪斯多夫距离，定义为：\n$$h(A,B)=\\max_{a\\in A}{\\min_{b\\in B}{\\left |a-b\\right|}}$$\n其中，$h(A,B)$为从集合$A$到集合$B$的单向豪斯多夫距离，$h(B,A)$为从集合$B$到集合$A$的单向豪斯多夫距离。\n分布距离 皮尔逊相关系数（Pearson Correlation Coefficient） 皮尔逊相关系数又被称为皮尔逊积矩相关技术（Pearson Product Moment Correlation Coefficient，PPMCC/PCC），用于度量两个变量$X$和$Y$之间的线性相关性。与上文提到的余弦相似度不同，皮尔逊相关系数不受平移变换的影响。对于$n$维空间中的任意两个分布$X$和$Y$的皮尔逊相关系数定义为：\n$$\\rho(X,Y)=\\frac{\\mathrm{cov}(X,Y)}{\\sigma(X)\\cdot\\sigma(Y)}=\\frac{\\mathrm{E}\\left[\\left(X-X_\\mu\\right)\\left(Y-Y_\\mu\\right)\\right]}{\\sigma(X)\\cdot\\sigma(Y)}$$\n皮尔逊相关系数与余弦相似度的关系为：\n$$\\rho(X,Y)=\\frac{\\mathrm{E}\\left[\\left(X-X_\\mu\\right)\\left(Y-Y_\\mu\\right)\\right]}{\\sigma(X)\\cdot\\sigma(Y)}=\\frac{\\displaystyle\\sum_{i=1}^{n}{\\left(X-X_\\mu\\right)\\cdot\\left(Y-Y_\\mu\\right)}}{\\left|X-X_\\mu\\right|\\cdot\\left|Y-Y_\\mu\\right|}=\\cos{\\left(X-X_\\mu,Y-Y_\\mu\\right)}$$\n在 Python 中用于计算任意两个分布间皮尔逊相关系数的代码如下：\nimport numpy as np def PearsonCorrelationCoefficient(x, y): x = np.array(x) y = np.array(y) x_ = x - np.mean(x) y_ = y - np.mean(y) return np.dot(x_, y_) / (np.linalg.norm(x_) * np.linalg.norm(y_)) 卡方度量（Chi-square Measure） $\\mathcal{X}^2$检验通常用于检验某一观测分布是否符合典型理论分布。若观测频数与期望频数差异越小，则$\\mathcal{X}^2$值越小；若观测频数与期望频数差异越大，则$\\mathcal{X}^2$值越大。因此，$\\mathcal{X}^2$值可以用于描述观测分布与理论分布的差异。对于$n$维空间中的任意两个分布$X$和$Y$的$\\mathcal{X}^2$统计量定义为：\n$$\\mathcal{X}^2=\\sum_{i=1}^{n}{\\frac{\\left(x_i-y_i\\right)^2}{y_i}}=\\sum_{i=1}^{k}{\\frac{\\left(x_i-n\\cdot p_i\\right)^2}{k\\cdot p_i}}$$\n其中，$x_i$为$X$在$i$的频数，$y_i$为$Y$在$i$的频数，$k$为总频数，$p_i$为$Y$在$i$的概率。在 Python 中用于计算任意两个分布间卡方度量的代码如下：\nimport numpy as np def ChiSquareMeasure(x, y): x = np.asarray(x, np.int32) y = np.asarray(y, np.int32) return np.sum(np.square(x - y) / y) 交叉熵（Cross Entropy） 交叉熵是香农信息论中的重要概念，用于度量两个分布之间的差异信息。对于$n$维空间中的任意两个分布$X$和$Y$的交叉熵定义为：\n$$H(X,Y)=-\\int_{p}{X(p)\\cdot Y(p)\\mathrm{d}p}$$\n若基于分布$X$对分布$X$进行编码，其编码长度的期望为：\n$$H(X)=-\\int_{p}{X(p)\\mathrm{d}p}$$\n若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$。在 Python 中用于计算任意两个分布间交叉熵的代码如下：\nimport numpy as np def CrossEntropy(x, y): return -np.sum(x * np.log(y)) KL 散度（Kullback-Leibler Divergence） KL 散度又被称为相对熵（Relative Entropy）或信息散度（Information Divergence），用于度量两个分布间的差异，对于$n$维空间中的任意两个分布$X$和$Y$的 KL 散度定义为：\n$$\\mathrm{KL}(X|Y)=\\int_{p}{X(p)\\cdot\\log{\\frac{X(p)}{Y(p)}}\\mathrm{d}p}$$\n若基于分布$X$对分布$X$进行编码，其编码长度的期望为：\n$$H(X)=-\\int_{p}{X(p)\\log{X(p)}\\mathrm{d}p}$$\n若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$，其多出的编码长度为：\n$$\\mathrm{KL}(X|Y)=H(X)-H(X,Y)$$\n在 Python 中用于计算任意两个分布间 KL 散度的代码如下：\nimport numpy as np def KullbackLeiblerDivergence(p, q): p = np.array(p) q = np.array(q) return np.sum(p * np.log(p / q)) JS 散度（Jensen-Shannon Divergence） JS 散度是 KL 散度的变体，解决了 KL 散度非对称的问题，对于$n$维空间中的任意两个分布$X$和$Y$的 JS 散度定义为：\n$$\\mathrm{JS}(X|Y)=\\frac{1}{2}\\cdot\\mathrm{KL}(X|\\frac{X+Y}{2})+\\frac{1}{2}\\cdot\\mathrm{KL}(Y|\\frac{X+Y}{2})$$\n在 Python 中用于计算任意两个分布间 JS 散度的代码如下：\nimport numpy as np def JensenShannonDivergence(p, q): p = np.array(p) q = np.array(q) return 0.5 * np.sum(p * np.log(2 * p/(p + q))) + 0.5 * np.sum(q * np.log(2 * q/(p + q))) 海林格距离（Hellinger Distance） 对于$n$维空间中的任意两个分布$X$和$Y$的海林格距离定义为：\n$$d={\\frac{1}{\\sqrt{2}}\\cdot\\sqrt{\\sum_{i=1}^{n}{\\left(\\sqrt{x_i}-\\sqrt{y_i}\\right)^2}}}$$\n在 Python 中用于计算任意两个分布间海林格距离的代码如下：\nimport numpy as np def HellingerDistance(p, q): p = np.array(p) q = np.array(q) return 1 / np.sqrt(2) * np.linalg.norm(np.sqrt(p) - np.sqrt(q)) α 散度（α Divergence） 对于$n$维空间中的任意两个分布$X$和$Y$的 α 散度被定义为：\n$$d=\\frac{4}{1-\\alpha^2}\\cdot\\left[1-\\int_p{X(p)^{(1+\\alpha)/2}\\cdot Y(p)^{(1-\\alpha)/2}}\\mathrm{d}x\\right]$$\n其中，$-\\infty\u003c\\alpha\u003c+\\infty$为连续参数。当$\\alpha\\rightarrow 1$时，α 散度退化为 KL 散度；当$\\alpha\\rightarrow 0$时，α 散度退化为海林格距离（仅相差常系数）。\nF 散度（F Divergence） 对于$n$维空间中的任意两个分布$X$和$Y$的 F 散度被定义为：\n$$\\mathrm{D}(X|Y)=\\int_p{Y(p)\\cdot f\\left[\\frac{X(p)}{Y(p)}\\right]\\mathrm{d}p}$$\n其中，函数$f(\\cdot)$需满足：（1）$f(\\cdot)$为凸函数；（2）$f(1)=0$。下表给出了$f(\\cdot)$取不同值时，F 散度对应的结果。\n散度 $f(\\cdot)$ 卡方距离 $(t-1)^2$ KL 散度 $x\\log{x}$ 逆 KL 散度 $-\\log{x}$ 海林格距离 $\\left(\\sqrt{x}-1\\right)^2$ α 散度 $4/(1-\\alpha)^2\\cdot\\left(1-x^{(1+\\alpha)/2}\\right)$ 在 Python 中用于计算任意两个分布间 F 散度（海林格距离）的代码如下：\nimport numpy as np def f(t): return t * np.log(t) def FDivergence(p, q): p = np.array(p) q = np.array(q) return np.sum(q * f(p / q)) 布雷格曼散度（Bregman Divergence） 对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的欧几里得距离定义为：\n$$d(x,y)=\\sqrt{\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}}$$\n将上式两侧平方后，得到：\n$$d^2(x,y)=\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}$$\n定义$=\\sum_{i=1}^{n}{x_i\\cdot y_i}$，$\\left|x\\right|=\\sqrt{}$，上式可改写为：\n$$d^2(x,y)=\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}==\\left|x\\right|^2-\\left(\\left|y\\right|^2+\u003c2y,x-y\u003e\\right)$$\n此处的距离即为欧几里得模函数和其在$x$处切线在$y$处的点估计差。推广该概念后，对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的布雷格曼散度定义为：\n$$d(x,y)=f(x)-\\left[f(y)+\u003c\\nabla f(x),x-y \u003e\\right]$$\n表给出了$f(\\cdot)$取不同值时，布雷格曼散度对应的结果。\n散度 $f(\\cdot)$ 平方损失 $x^2$ / $x\\log{x}$ Logistic 损失 $x\\log{x}+(1-x)\\log{(1-x)}$ Itakura-Saito 距离 $-\\log{x}$ / $e^x$ 平方欧几里得距离 $\\left|x\\right|^2$ 马氏距离 $\\mathbf{X}^TA\\mathbf{X}$ KL 散度 $\\sum_{i=1}^{n}{x_i\\log{x_i}}$ Wasserstein 距离（Wasserstein Distance） Wasserstein 距离被称为推土机距离，用于表示两个分布的相似度。Wasserstein 距离定义为把分布$X$转变成分布$Y$所需移动的平均距离的最小值，如下图所示：\n图 5：Wasserstein 距离（Wasserstein Distance）\n对于$n$维空间中的任意两个分布$X$和$Y$的 Wasserstein 距离定义为：\n$$d=\\inf_{\\gamma\\sim\\Pi(X,Y)}{\\mathrm{E}_{p,q\\sim\\gamma}\\left|p-q\\right|}$$\n其中，$\\Pi(X,Y)$为分布$X$和分布$Y$构成的联合分布的集合，$\\gamma$为$\\Pi(X,Y)$中任意分布，$p$和$q$是分布$\\gamma$中的样本。\n巴氏距离（Bhattacharyya Distance） 巴氏系数可以用来度量两个分布的相似性，对于$n$维空间中的任意两个分布$X$和$Y$的巴氏系数定义为：\n$$c_b=\\int_{p}{\\sqrt{X(p)\\cdot Y(p)}\\mathrm{d}p}$$\n巴氏距离定义为：\n$$d_b=-\\ln{c_b}$$\n需要注意的是，海林格距离$d=\\sqrt{1-d_b}$。在 Python 中用于计算任意两个分布间巴氏距离的代码如下：\nimport numpy as np def BhattacharyyaCoefficient(p,q): p = np.array(p) q = np.array(q) return np.sum(np.sqrt(p * q)) def HellingerDistance(p, q): return np.sqrt(1 - BhattacharyyaCoefficient(p, q)) def BhattacharyyaDistance(p, q): return np.log(BhattacharyyaCoefficient(p, q)) 最大均值差异（Maximum Mean Discrepancy） 最大均值差异是迁移学习领域中最为广泛使用的一种损失函数，它度量了再生希尔伯特空间中两个不同分布间的距离。通过在样本空间寻找连续函数$f:X\\rightarrow R$随机投影后，分别求这两个分布在$f$上函数值的均值，并通过做差得到均值差异（Mean Discrepancy）。最大化均值差异即为寻找一个$f$使得均值差异最大。对于$n$维空间中的任意两个分布$X$和$Y$的最大均值差异定义为：\n$$d=\\sup_{|f|_\\mathrm{H} \\leq 1}{\\mathrm{E}_p[f(X)]-\\mathrm{E}_q[f(Y)]}$$\n点间互信息（Pointwise Mutual Information） 点间互信息用来衡量两个分布的相关性，对于$n$维空间中的任意两个分布$X$和$Y$的点间互信息定义为：\n$$d=\\log{\\frac{p(X,Y)}{p(X)\\cdot p(Y)}}=\\log{\\frac{p(X|Y)}{p(X)}}=\\log{\\frac{p(Y|X)}{p(Y)}}$$\n若$X$和$Y$不相关，则$P(X,Y)=P(X)\\cdot P(Y)$。\n","wordCount":"7820","inLanguage":"zh","datePublished":"2023-08-18T11:27:00+08:00","dateModified":"2023-08-18T11:27:00+08:00","author":{"@type":"Person","name":"Kai Wang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/"},"publisher":{"@type":"Organization","name":"退思轩","logo":{"@type":"ImageObject","url":"https://kwang.life/images/others/site-favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><header class=header><nav class=nav><div class=logo><a href=https://kwang.life accesskey=h title="退思轩 (Alt + H)"><img src=https://kwang.life/images/others/site-favicon.png alt aria-label=logo height=35>退思轩</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kwang.life/ title=🏠首页><span>🏠首页</span></a></li><li><a href=https://kwang.life/archives/ title=📔归档><span>📔归档</span></a></li><li><a href=https://kwang.life/tags/ title=🏷️标签><span>🏷️标签</span></a></li><li><a href=https://kwang.life/search/ title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://kwang.life/categories/ title=🗂️分类><span>🗂️分类</span></a></li><li><a href=https://kwang.life/friends/ title=🌏邻居><span>🌏邻居</span></a></li><li><a href=https://kwang.life/about/ title=📜关于><span>📜关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>机器学习中常用的距离度量汇总</h1><div class=post-meta><span title='2023-08-18 11:27:00 +0800 +0800'>2023年8月18日</span>&nbsp;·&nbsp;16 分钟&nbsp;·&nbsp;7820 字&nbsp;·&nbsp;Kai Wang&nbsp;·&nbsp;<span id=busuanzi_container_site_pv>本文阅读量&nbsp;<span id=busuanzi_value_page_pv></span>&nbsp;次</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e8%b7%9d%e7%a6%bb%e7%9a%84%e5%ae%9a%e4%b9%89 aria-label=距离的定义>距离的定义</a></li><li><a href=#%e7%a9%ba%e9%97%b4%e8%b7%9d%e7%a6%bb aria-label=空间距离>空间距离</a><ul><li><a href=#%e6%ac%a7%e5%87%a0%e9%87%8c%e5%be%97%e8%b7%9d%e7%a6%bbeuclidean-distance aria-label="欧几里得距离（Euclidean Distance）">欧几里得距离（Euclidean Distance）</a></li><li><a href=#%e6%9b%bc%e5%93%88%e9%a1%bf%e8%b7%9d%e7%a6%bbmanhattan-distance aria-label="曼哈顿距离（Manhattan Distance）">曼哈顿距离（Manhattan Distance）</a></li><li><a href=#%e5%88%87%e6%af%94%e9%9b%aa%e5%a4%ab%e8%b7%9d%e7%a6%bbchebyschev-distance aria-label="切比雪夫距离（Chebyschev Distance）">切比雪夫距离（Chebyschev Distance）</a></li><li><a href=#%e9%97%b5%e5%8f%af%e5%a4%ab%e6%96%af%e5%9f%ba%e8%b7%9d%e7%a6%bbminkowski-distance aria-label="闵可夫斯基距离（Minkowski Distance）">闵可夫斯基距离（Minkowski Distance）</a></li><li><a href=#%e6%a0%87%e5%87%86%e5%8c%96%e6%ac%a7%e5%87%a0%e9%87%8c%e5%be%97%e8%b7%9d%e7%a6%bbstandardized-euclidean-distance aria-label="标准化欧几里得距离（Standardized Euclidean Distance）">标准化欧几里得距离（Standardized Euclidean Distance）</a></li><li><a href=#%e9%a9%ac%e6%b0%8f%e8%b7%9d%e7%a6%bbmahalanobis-distance aria-label="马氏距离（Mahalanobis Distance）">马氏距离（Mahalanobis Distance）</a></li><li><a href=#%e5%85%b0%e6%b0%8f%e8%b7%9d%e7%a6%bblance-and-williams-distance aria-label="兰氏距离（Lance and Williams Distance）">兰氏距离（Lance and Williams Distance）</a></li><li><a href=#%e4%bd%99%e5%bc%a6%e7%9b%b8%e4%bc%bc%e5%ba%a6cosine-similarity aria-label="余弦相似度（Cosine Similarity）">余弦相似度（Cosine Similarity）</a></li><li><a href=#%e6%b5%8b%e5%9c%b0%e8%b7%9d%e7%a6%bbgeodesic-distance aria-label="测地距离（Geodesic Distance）">测地距离（Geodesic Distance）</a></li><li><a href=#%e5%b8%83%e9%9b%b7%e6%9f%af%e8%92%82%e6%96%af%e8%b7%9d%e7%a6%bbbray-curtis-distance aria-label="布雷柯蒂斯距离（Bray Curtis Distance）">布雷柯蒂斯距离（Bray Curtis Distance）</a></li><li><a href=#%e5%8d%8a%e6%ad%a3%e7%9f%a2%e8%b7%9d%e7%a6%bbhaversine-distance aria-label="半正矢距离（Haversine Distance）">半正矢距离（Haversine Distance）</a></li></ul></li><li><a href=#%e5%ad%97%e7%ac%a6%e8%b7%9d%e7%a6%bb aria-label=字符距离>字符距离</a><ul><li><a href=#%e6%b1%89%e6%98%8e%e8%b7%9d%e7%a6%bbhamming-distance aria-label="汉明距离（Hamming Distance）">汉明距离（Hamming Distance）</a></li><li><a href=#%e8%8e%b1%e6%96%87%e6%96%af%e5%9d%a6%e8%b7%9d%e7%a6%bblevenshtein-distance aria-label="莱文斯坦距离（Levenshtein Distance）">莱文斯坦距离（Levenshtein Distance）</a></li><li><a href=#%e5%bd%92%e4%b8%80%e5%8c%96-google-%e8%b7%9d%e7%a6%bbnormalized-google-distance aria-label="归一化 Google 距离（Normalized Google Distance）">归一化 Google 距离（Normalized Google Distance）</a></li><li><a href=#jaro-winkler-%e7%9b%b8%e4%bc%bc%e5%ba%a6jaro-winkler-similarity aria-label="Jaro-Winkler 相似度（Jaro-Winkler Similarity）">Jaro-Winkler 相似度（<strong>J</strong>aro-Winkler Sim<strong>ilarity</strong>）</a></li><li><a href=#%e6%9d%8e%e8%b7%9d%e7%a6%bblee-distance aria-label="李距离（Lee Distance）">李距离（Lee Distance）</a></li></ul></li><li><a href=#%e9%9b%86%e5%90%88%e8%b7%9d%e7%a6%bb aria-label=集合距离>集合距离</a><ul><li><a href=#%e6%9d%b0%e5%8d%a1%e5%be%b7%e7%9b%b8%e4%bc%bc%e7%b3%bb%e6%95%b0jaccard-similarity-coefficient aria-label="杰卡德相似系数（Jaccard Similarity Coefficient）">杰卡德相似系数（Jaccard Similarity Coefficient）</a></li><li><a href=#%e5%a5%a5%e5%a5%87%e4%ba%9a%e7%b3%bb%e6%95%b0ochiia-coefficient aria-label="奥奇亚系数（Ochiia Coefficient）">奥奇亚系数（Ochiia Coefficient）</a></li><li><a href=#%e6%88%b4%e6%96%af%e7%b3%bb%e6%95%b0dice-coefficient aria-label="戴斯系数（Dice Coefficient）">戴斯系数（Dice Coefficient）</a></li><li><a href=#%e8%b1%aa%e6%96%af%e5%a4%9a%e5%a4%ab%e8%b7%9d%e7%a6%bbhausdorff-distance aria-label="豪斯多夫距离（Hausdorff Distance）">豪斯多夫距离（Hausdorff Distance）</a></li></ul></li><li><a href=#%e5%88%86%e5%b8%83%e8%b7%9d%e7%a6%bb aria-label=分布距离>分布距离</a><ul><li><a href=#%e7%9a%ae%e5%b0%94%e9%80%8a%e7%9b%b8%e5%85%b3%e7%b3%bb%e6%95%b0pearson-correlation-coefficient aria-label="皮尔逊相关系数（Pearson Correlation Coefficient）">皮尔逊相关系数（Pearson Correlation Coefficient）</a></li><li><a href=#%e5%8d%a1%e6%96%b9%e5%ba%a6%e9%87%8fchi-square-measure aria-label="卡方度量（Chi-square Measure）">卡方度量（Chi-square Measure）</a></li><li><a href=#%e4%ba%a4%e5%8f%89%e7%86%b5cross-entropy aria-label="交叉熵（Cross Entropy）">交叉熵（Cross Entropy）</a></li><li><a href=#kl-%e6%95%a3%e5%ba%a6kullback-leibler-divergence aria-label="KL 散度（Kullback-Leibler Divergence）">KL 散度（Kullback-Leibler Divergence）</a></li><li><a href=#js-%e6%95%a3%e5%ba%a6jensen-shannon-divergence aria-label="JS 散度（Jensen-Shannon Divergence）">JS 散度（Jensen-Shannon Divergence）</a></li><li><a href=#%e6%b5%b7%e6%9e%97%e6%a0%bc%e8%b7%9d%e7%a6%bbhellinger-distance aria-label="海林格距离（Hellinger Distance）">海林格距离（Hellinger Distance）</a></li><li><a href=#%ce%b1-%e6%95%a3%e5%ba%a6%ce%b1-divergence aria-label="α 散度（α Divergence）">α 散度（α Divergence）</a></li><li><a href=#f-%e6%95%a3%e5%ba%a6f-divergence aria-label="F 散度（F Divergence）">F 散度（F Divergence）</a></li><li><a href=#%e5%b8%83%e9%9b%b7%e6%a0%bc%e6%9b%bc%e6%95%a3%e5%ba%a6bregman-divergence aria-label="布雷格曼散度（Bregman Divergence）">布雷格曼散度（Bregman Divergence）</a></li><li><a href=#wasserstein-%e8%b7%9d%e7%a6%bbwasserstein-distance aria-label="Wasserstein 距离（Wasserstein Distance）">Wasserstein 距离（Wasserstein Distance）</a></li><li><a href=#%e5%b7%b4%e6%b0%8f%e8%b7%9d%e7%a6%bbbhattacharyya-distance aria-label="巴氏距离（Bhattacharyya Distance）">巴氏距离（Bhattacharyya Distance）</a></li><li><a href=#%e6%9c%80%e5%a4%a7%e5%9d%87%e5%80%bc%e5%b7%ae%e5%bc%82maximum-mean-discrepancy aria-label="最大均值差异（Maximum Mean Discrepancy）">最大均值差异（Maximum Mean Discrepancy）</a></li><li><a href=#%e7%82%b9%e9%97%b4%e4%ba%92%e4%bf%a1%e6%81%afpointwise-mutual-information aria-label="点间互信息（Pointwise Mutual Information）">点间互信息（Pointwise Mutual Information）</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener('DOMContentLoaded',function(){checkTocPosition(),elements=document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]'),activeElement=elements[0];const e=encodeURI(activeElement.getAttribute('id')).toLowerCase();document.querySelector(`.inner ul li a[href="#${e}"]`).classList.add('active')},!1),window.addEventListener('resize',function(){checkTocPosition()},!1),window.addEventListener('scroll',()=>{elements&&(activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute('id')).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add('active'):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove('active')}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue('--gap'),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=距离的定义>距离的定义<a hidden class=anchor aria-hidden=true href=#距离的定义>#</a></h2><p>在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于<strong>距离度量函数</strong>$d(x,y)$，其需要满足如下性质：</p><ul><li><strong>非负性</strong>：$d(x,y)\geq 0$</li><li><strong>同一性</strong>：$d(x,y)=0\Leftrightarrow x=y$</li><li><strong>对称性</strong>：$d(x,y)=d(y,x)$</li><li><strong>三角不等式</strong>：$d(x,y)\leq d(x,z)+d(z,y)$</li></ul><p>根据样本特征空间的不同，我们把度量的距离分为：<strong>空间距离</strong>、<strong>字符距离</strong>、<strong>集合距离</strong>、<strong>分布距离</strong>。</p><h2 id=空间距离>空间距离<a hidden class=anchor aria-hidden=true href=#空间距离>#</a></h2><h3 id=欧几里得距离euclidean-distance>欧几里得距离（Euclidean Distance）<a hidden class=anchor aria-hidden=true href=#欧几里得距离euclidean-distance>#</a></h3><p>欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\mathcal{L}_2$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间欧几里得距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>EuclideanDistance</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>square(x <span style=color:#f92672>-</span> y)))
</span></span></code></pre></div><h3 id=曼哈顿距离manhattan-distance>曼哈顿距离（Manhattan Distance）<a hidden class=anchor aria-hidden=true href=#曼哈顿距离manhattan-distance>#</a></h3><p>曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\mathcal{L}_1$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的曼哈顿距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left|x_i-y_i\right|}}$$</p><p>下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。</p><figure><div class=fancybox><a data-fancybox=gallery data-src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/7445695fcba340a9a7b62646bec99c2b.png data-caption="图 1：曼哈顿距离（Manhattan Distance）" href=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/7445695fcba340a9a7b62646bec99c2b.png><img loading=lazy src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/7445695fcba340a9a7b62646bec99c2b.png alt="图 1：曼哈顿距离（Manhattan Distance）"></a></div><figcaption><p>图 1：曼哈顿距离（Manhattan Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间曼哈顿距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ManhattanDistance</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>abs(x <span style=color:#f92672>-</span> y))
</span></span></code></pre></div><h3 id=切比雪夫距离chebyschev-distance>切比雪夫距离（Chebyschev Distance）<a hidden class=anchor aria-hidden=true href=#切比雪夫距离chebyschev-distance>#</a></h3><p>切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\mathcal{L}_\infty$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的切比雪夫距离定义为：</p><p>$$d=\max\left|x_i-y_i\right|$$</p><p>以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。</p><figure><div class=fancybox><a data-fancybox=gallery data-src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/ce4d7254fa21da30b3560ca8bc2a7edb.png data-caption="图 2：切比雪夫距离（Chebyschev Distance）" href=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/ce4d7254fa21da30b3560ca8bc2a7edb.png><img loading=lazy src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/ce4d7254fa21da30b3560ca8bc2a7edb.png alt="图 2：切比雪夫距离（Chebyschev Distance）"></a></div><figcaption><p>图 2：切比雪夫距离（Chebyschev Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间切比雪夫距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ChebyshevDistance</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>max(np<span style=color:#f92672>.</span>abs(x <span style=color:#f92672>-</span> y))
</span></span></code></pre></div><h3 id=闵可夫斯基距离minkowski-distance>闵可夫斯基距离（Minkowski Distance）<a hidden class=anchor aria-hidden=true href=#闵可夫斯基距离minkowski-distance>#</a></h3><p>闵可夫斯基距离并非一种新型的距离度量方式，而是一种对于多种不同距离度量的概括性表述。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的闵可夫斯基距离定义为：</p><p>$$d=\left(\sum_{i=1}^{n}{\left|x_i-y_i\right|^p}\right)^{1/p}$$</p><p>其中，$p$为闵可夫斯基距离参数。当$p=1$时，闵可夫斯基距离退化为曼哈顿距离；当$p=2$时，闵可夫斯基距离退化为欧几里得距离；当$p\rightarrow\infty$时，闵可夫斯基距离退化为切比雪夫距离，如下图所示：</p><figure><div class=fancybox><a data-fancybox=gallery data-src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/4979bb4823e6d2ff079d1ff92de69978.png data-caption="图 3：闵可夫斯基距离（Minkowski Distance）" href=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/4979bb4823e6d2ff079d1ff92de69978.png><img loading=lazy src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/4979bb4823e6d2ff079d1ff92de69978.png alt="图 3：闵可夫斯基距离（Minkowski Distance）"></a></div><figcaption><p>图 3：闵可夫斯基距离（Minkowski Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间闵可夫斯基距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>MinkowskiDistance</span>(x, y, p):
</span></span><span style=display:flex><span>    zipped_coordinate <span style=color:#f92672>=</span> zip(x, y)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> math<span style=color:#f92672>.</span>pow(np<span style=color:#f92672>.</span>sum(math<span style=color:#f92672>.</span>pow(np<span style=color:#f92672>.</span>abs(x <span style=color:#f92672>-</span> y), p)), <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> p)
</span></span></code></pre></div><h3 id=标准化欧几里得距离standardized-euclidean-distance>标准化欧几里得距离（Standardized Euclidean Distance）<a hidden class=anchor aria-hidden=true href=#标准化欧几里得距离standardized-euclidean-distance>#</a></h3><p>标准化欧几里得距离是将欧式空间中任意两点的分量都“标准化”到均值、方差一致的区间，记每个分量的均值为$\mu$，方差为$\sigma_i^2$，“标准化”结果为$X^*$：</p><p>$$X^*=\frac{X-\mu}{\sigma^2}$$</p><p>对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的标准化欧几里得距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left(\frac{x_i-y_i}{\sigma_i^2}\right)^2}}$$</p><p>如果将$1/\sigma^2$看做权重，则标准化欧几里得距离可以被认为是加权欧几里得距离。</p><p>在 <code>Python</code> 中用于计算任意两点间标准化欧几里得距离的代码如下（需要注意 <code>sigma ≠ 0</code>）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>StandardizedEuclideanDistance</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack([x,y])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    sigma <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>var(X, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, ddof<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sqrt(((x <span style=color:#f92672>-</span> y) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span>sigma)<span style=color:#f92672>.</span>sum())
</span></span></code></pre></div><h3 id=马氏距离mahalanobis-distance>马氏距离（Mahalanobis Distance）<a hidden class=anchor aria-hidden=true href=#马氏距离mahalanobis-distance>#</a></h3><p>马氏距离由印度统计学家<a href=https://en.wikipedia.org/wiki/Prasanta_Chandra_Mahalanobis>马哈拉诺比斯（P. C. Mahalanobis）</a>提出，用于表示数据的协方差距离，它可以有效地表示两个未知样本集之间的相似度。对于$n$维空间中的任意两个样本集$\textbf{X}$和$\textbf{Y}$的马氏距离定义为：</p><p>$$d=\sqrt{\left(\textbf{X}-\textbf{Y}\right)^T\Sigma^{-1}\left(\textbf{X}-\textbf{Y}\right)}$$</p><p>其中，$\Sigma$为样本集$X$和$Y$的协方差矩阵。若协方差矩阵为单位矩阵，则马氏距离退化为欧几里得距离；若协方差矩阵为对角矩阵，则马氏距离退化为标准化欧几里得距离。</p><p>需要注意的是，马氏距离的计算需要确保$\Sigma$的逆矩阵存在，否则可以直接采用欧几里得距离进行计算。此外，马氏距离不受量纲的影响，它可以排除变量之间相关性的干扰，但同时也夸大了微小变化量的作用。例如，若将两个相同的样本放入两个不同的总体中，经计算的到的马氏距离也是不同的（除非它们的$\Sigma$恰巧相同）。</p><p>在 <code>Python</code> 中用于计算任意两个样本集间马氏距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>MahalanobisDistance</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>vstack([x,y])
</span></span><span style=display:flex><span>    X_T <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>    sigma_inverse <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(np<span style=color:#f92672>.</span>cov(X))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    d <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, X_T<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, X_T<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>            delta <span style=color:#f92672>=</span> X_T[i] <span style=color:#f92672>-</span> X_T[j]
</span></span><span style=display:flex><span>            d<span style=color:#f92672>.</span>append(np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>dot(delta,sigma_inverse),delta<span style=color:#f92672>.</span>T)))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> d
</span></span></code></pre></div><h3 id=兰氏距离lance-and-williams-distance>兰氏距离（Lance and Williams Distance）<a hidden class=anchor aria-hidden=true href=#兰氏距离lance-and-williams-distance>#</a></h3><p>兰氏距离又被称为堪培拉距离（Canberra Distance），可以理解为加权曼哈顿距离。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的兰氏距离定义为：</p><p>$$d=\sum_{i=1}^{n}{\frac{\left|x_i-y_i\right|}{\left|x_i\right|+\left|y_i\right|}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间兰氏距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>CanberraDistance</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    d <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(x)):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> x[i] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> y[i] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            d <span style=color:#f92672>+=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            d <span style=color:#f92672>+=</span> abs(x[i] <span style=color:#f92672>-</span> y[i]) <span style=color:#f92672>/</span> (abs(x[i]) <span style=color:#f92672>+</span> abs(y[i]))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> d
</span></span></code></pre></div><h3 id=余弦相似度cosine-similarity>余弦相似度（Cosine Similarity）<a hidden class=anchor aria-hidden=true href=#余弦相似度cosine-similarity>#</a></h3><p>在几何学中，通常采用余弦相似度来度量两个向量间的夹角，其取值为$[-1,1]$。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的余弦相似度定义为：</p><p>$$\cos{\left(\vec{x},\vec{y}\right)}=\frac{\vec{x}\cdot\vec{y}}{\left|\vec{x}\right|\cdot\left|\vec{y}\right|}=\frac{\displaystyle\sum_{i=1}^{n}{x_i\cdot y_i}}{\sqrt{\displaystyle\sum_{i=1}^{n}{x_i^2}}\cdot\sqrt{\displaystyle\sum_{i=1}^{n}{y_i^2}}}$$</p><p>当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow0$时，两向量完全正交；当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow1$时，两向量完全重合；当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow -1$时，两向量完全相反。在 <code>Python</code> 中用于计算任意两点间余弦相似度的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>CosineDistance</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>dot(x,y) <span style=color:#f92672>/</span> (np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(x) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(y))
</span></span></code></pre></div><h3 id=测地距离geodesic-distance>测地距离（Geodesic Distance）<a hidden class=anchor aria-hidden=true href=#测地距离geodesic-distance>#</a></h3><p>测地距离原指球体表面上两点间的最短距离，后来被推广到其它领域。在图论中，测地距离为两顶点间的最短路径；在欧式空间中，测地距离为欧几里得距离；在非欧空间中，测地距离为连接两点间的最短圆弧。如下图所示：</p><figure><div class=fancybox><a data-fancybox=gallery data-src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/82e6075c65ebe7716be1da43ecea9eb6.jpg data-caption="图 4：测地距离（Geodesic Distance）" href=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/82e6075c65ebe7716be1da43ecea9eb6.jpg><img loading=lazy src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/82e6075c65ebe7716be1da43ecea9eb6.jpg alt="图 4：测地距离（Geodesic Distance）"></a></div><figcaption><p>图 4：测地距离（Geodesic Distance）</p></figcaption></figure><h3 id=布雷柯蒂斯距离bray-curtis-distance>布雷柯蒂斯距离（Bray Curtis Distance）<a hidden class=anchor aria-hidden=true href=#布雷柯蒂斯距离bray-curtis-distance>#</a></h3><p>布雷柯蒂斯距离主要用于生态学和环境科学领域，用于计算不同样本间的差异。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的布雷柯蒂斯距离定义为：</p><p>$$d=\frac{\displaystyle \sum_{i=1}^{n}{\left|x_i-y_i\right|}}{\displaystyle \sum_{i=1}^{n}{x_i}+\sum_{i=1}^{n}{y_i}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间布雷柯蒂斯距离的代码如下：</p><pre tabindex=0><code class=language-text-plain data-lang=text-plain>import numpy as np

def BrayCurtisDistance(x, y):
    x = np.array(x)
    y = np.array(y)
    return np.sum(np.abs(x - y)) / (np.sum(x) + np.sum(y))
</code></pre><h3 id=半正矢距离haversine-distance>半正矢距离（Haversine Distance）<a hidden class=anchor aria-hidden=true href=#半正矢距离haversine-distance>#</a></h3><p>半正矢距离用于计算任意两经纬点间的距离，对于空间中的任意两经纬点$A(\mathrm{lon}_1,\mathrm{lat}_1)$和$B(\mathrm{lon}_2,\mathrm{lat}_2)$的半正矢距离定义为：</p><p>$$d=2r\cdot\arcsin{\sqrt{\sin^2{\frac{\mathrm{lat}_2-\mathrm{lat}_1}{2}}+\cos{(\mathrm{lat}_1)}\cos{(\mathrm{lat}_2)}\sin^2{\frac{\mathrm{lon}_2-\mathrm{lon}_1}{2}}}}$$</p><p>其中，$r$为半径。在 <code>Python</code> 中用于计算任意两经纬点间半正矢距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>HaversineDistance</span>(lon1, lat1, lon2, lat2):
</span></span><span style=display:flex><span>    lon1, lat1, lon2, lat2 <span style=color:#f92672>=</span> map(np<span style=color:#f92672>.</span>radians, [lon1, lat1, lon2, lat2])
</span></span><span style=display:flex><span>    c <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>6367</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1000</span> <span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>arcsin(np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>sin((lat2 <span style=color:#f92672>-</span> lat1) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2.0</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>cos(lat1) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>cos(lat2) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sin((lon2 <span style=color:#f92672>-</span> lon1) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2.0</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>))
</span></span></code></pre></div><h2 id=字符距离>字符距离<a hidden class=anchor aria-hidden=true href=#字符距离>#</a></h2><h3 id=汉明距离hamming-distance>汉明距离（Hamming Distance）<a hidden class=anchor aria-hidden=true href=#汉明距离hamming-distance>#</a></h3><p>汉明距离由两个字符串对应位不同的数量决定，通常用于数据传输差错控制编码领域。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的汉明距离定义为：</p><p>$$d=\sum_{i=0}^{n}{x_i\otimes y_i}$$</p><p>汉明距离也可以理解为将$A$变为$B$的最小操作次数。在 <code>Python</code> 中用于计算任意两个字符串间汉明距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>HammingDistance</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sum(x_ch <span style=color:#f92672>!=</span> y_ch <span style=color:#66d9ef>for</span> x_ch, y_ch <span style=color:#f92672>in</span> zip(x, y))
</span></span></code></pre></div><h3 id=莱文斯坦距离levenshtein-distance>莱文斯坦距离（Levenshtein Distance）<a hidden class=anchor aria-hidden=true href=#莱文斯坦距离levenshtein-distance>#</a></h3><p>莱文斯坦距离又被称为编辑距离（Edit Distance），用于度量两个字符串之间的差异，定义为：将字符串$A$转化为字符串$B$所需的最少单字符编辑（插入、删除或替换）次数。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$，$A$的前$i$个字符和$B$的前$j$个字符的莱文斯坦距离定义为：</p><p>$$d(i,j) = \begin{align*} \left{\begin{matrix} \max{\left(i,j\right)}, & \min{\left(i,j\right)}=0 \ \min{\left[d(i-1,j),d(i,j-1), d(i,j)\right]} + I(i,j), & \min{\left(i,j\right)\neq 0} \end{matrix}\right. \end{align*}$$</p><p>其中，$I(\cdot)$为指示函数，当$x_i=y_j$时，$I(i,j)=0$；当$x_i\neq y_j$时，$I(i,j)=1$。在 <code>Python</code> 中用于计算任意两个字符串间莱文斯坦距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>LevenshteinDistance</span>(x, y):
</span></span><span style=display:flex><span>    dp <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((len(x) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>,len(y) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(x) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        dp[i][<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> i    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(len(y) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        dp[<span style=color:#ae81ff>0</span>][j] <span style=color:#f92672>=</span> j
</span></span><span style=display:flex><span>     
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, len(x) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, len(y) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>            delta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> x[i<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#f92672>==</span> y[j<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>] <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            dp[i][j] <span style=color:#f92672>=</span> min(dp[i <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>][j <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> delta, min(dp[i<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>][j] <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, dp[i][j <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> int(dp[len(x)][len(y)])
</span></span></code></pre></div><h3 id=归一化-google-距离normalized-google-distance>归一化 Google 距离（Normalized Google Distance）<a hidden class=anchor aria-hidden=true href=#归一化-google-距离normalized-google-distance>#</a></h3><p>归一化 Google 距离是由给定一组关键词集合的 Google 搜索引擎所返回的命中数量决定的，它是一种语义相似度量方法。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的归一化 Google 距离定义为：</p><p>$$d=\frac{\max{\left[\log{f(A)},\log{f(B)}\right]}-\log{f(A,B)}}{\log{M}-\min{\left[\log{f(A)}, \log{f(B)}\right]}}$$</p><p>其中，$M$为 Google 搜索引擎所返回的网页总数；$f(x)$和$f(y)$分别为 Google 搜索引擎返回关于字符串$A$和字符串$B$的命中数量；$f(A,B)$为 Google 搜索引擎返回关于字符串$A$和字符串$B$同时出现的命中数量。</p><h3 id=jaro-winkler-相似度jaro-winkler-similarity>Jaro-Winkler 相似度（<strong>J</strong>aro-Winkler Sim<strong>ilarity</strong>）<a hidden class=anchor aria-hidden=true href=#jaro-winkler-相似度jaro-winkler-similarity>#</a></h3><p>Jaro 相似度用于评估两个字符串间的相似度，对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的 Jaro 相似度定义为：</p><p>$$d_j=\frac{1}{3}\cdot\left(\frac{m}{|A|}+\frac{m}{|B|}+\frac{m-t}{m}\right)$$</p><p>其中，$m$是匹配的字符数，$t$是替换的字符数。</p><p>Jaro-Winkler 相似度在 Jaro 相似度的基础上引入了前缀的概念，对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的 Jaro-Winkler 相似度定义为：</p><p>$$d_w=d_j+l\cdot p\cdot(1-d_j)$$</p><p>其中，$l$为字符串$A$和字符串$B$的共同前缀的字符数；$p$为缩放因子（常取$p=0.1$）。</p><h3 id=李距离lee-distance>李距离（Lee Distance）<a hidden class=anchor aria-hidden=true href=#李距离lee-distance>#</a></h3><p>李距离是编码理论中用于描述字符串距离的方案，对于使用包含$q$个字母的字母表且长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的李距离定义为：</p><p>$$d=\sum_{i=0}^{n}{\min{\left(\left|x_i-y_i\right|,q-\left|x_i-y_i\right|\right)}}$$</p><p>当$q=2$或$q=3$时，李距离退化为汉明距离。</p><h2 id=集合距离>集合距离<a hidden class=anchor aria-hidden=true href=#集合距离>#</a></h2><h3 id=杰卡德相似系数jaccard-similarity-coefficient>杰卡德相似系数（Jaccard Similarity Coefficient）<a hidden class=anchor aria-hidden=true href=#杰卡德相似系数jaccard-similarity-coefficient>#</a></h3><p>杰卡德相似系数是指两个集合$A$和$B$中相同元素在所有元素中的占比，定义为$J(A,B)$：</p><p>$$J(A,B)=\frac{\left|A\cap B\right|}{\left|A\cup B\right|}$$</p><p>杰卡德距离（Jaccard Distance）是指两个集合$A$和$B$中不同元素在所有元素中的占比，定义为$J_\delta(A,B)$：</p><p>$$J_\delta(A,B)=1-J(A,B)=1-\frac{\left|A\cap B\right|}{\left|A\cup B\right|}$$</p><p>在 <code>Python</code> 中用于计算任意两个集合间杰卡德相似系数的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>JaccardSimilarityCoefficient</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(x, np<span style=color:#f92672>.</span>int32)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(y, np<span style=color:#f92672>.</span>int32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>double(np<span style=color:#f92672>.</span>bitwise_and((x <span style=color:#f92672>!=</span> y), np<span style=color:#f92672>.</span>bitwise_or(x <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>, y <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>))<span style=color:#f92672>.</span>sum()) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>double(np<span style=color:#f92672>.</span>bitwise_or(x <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>, y <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>sum())
</span></span></code></pre></div><h3 id=奥奇亚系数ochiia-coefficient>奥奇亚系数（Ochiia Coefficient）<a hidden class=anchor aria-hidden=true href=#奥奇亚系数ochiia-coefficient>#</a></h3><p>奥奇亚系数是指两个集合$A$和$B$中相同元素与两集合大小几何平均值的比值，定义为$K(A,B)$：</p><p>$$K(A,B)=\frac{\left|A\cap B\right|}{\sqrt{\left|A\right|\times\left|B\right|}}$$</p><h3 id=戴斯系数dice-coefficient>戴斯系数（Dice Coefficient）<a hidden class=anchor aria-hidden=true href=#戴斯系数dice-coefficient>#</a></h3><p>戴斯系数除了可以用来衡量两个集合间的距离，还可以用来衡量两个字符串间的距离，定义为$D(A,B)$：</p><p>$$D(A,B)=\frac{2\left|A\cap B\right|}{\left|A\right|+\left|B\right|}$$</p><h3 id=豪斯多夫距离hausdorff-distance>豪斯多夫距离（Hausdorff Distance）<a hidden class=anchor aria-hidden=true href=#豪斯多夫距离hausdorff-distance>#</a></h3><p>豪斯多夫距离用于度量两个集合$A$和$B$间的距离，定义为$H(A,B)$：</p><p>$$H(A,B)=\max{\left[h(A,B),h(B,A)\right]}$$</p><p>其中，$h(\cdot)$为双向豪斯多夫距离，定义为：</p><p>$$h(A,B)=\max_{a\in A}{\min_{b\in B}{\left |a-b\right|}}$$</p><p>其中，$h(A,B)$为从集合$A$到集合$B$的单向豪斯多夫距离，$h(B,A)$为从集合$B$到集合$A$的单向豪斯多夫距离。</p><h2 id=分布距离>分布距离<a hidden class=anchor aria-hidden=true href=#分布距离>#</a></h2><h3 id=皮尔逊相关系数pearson-correlation-coefficient>皮尔逊相关系数（Pearson Correlation Coefficient）<a hidden class=anchor aria-hidden=true href=#皮尔逊相关系数pearson-correlation-coefficient>#</a></h3><p>皮尔逊相关系数又被称为皮尔逊积矩相关技术（Pearson Product Moment Correlation Coefficient，PPMCC/PCC），用于度量两个变量$X$和$Y$之间的线性相关性。与上文提到的余弦相似度不同，皮尔逊相关系数不受平移变换的影响。对于$n$维空间中的任意两个分布$X$和$Y$的皮尔逊相关系数定义为：</p><p>$$\rho(X,Y)=\frac{\mathrm{cov}(X,Y)}{\sigma(X)\cdot\sigma(Y)}=\frac{\mathrm{E}\left[\left(X-X_\mu\right)\left(Y-Y_\mu\right)\right]}{\sigma(X)\cdot\sigma(Y)}$$</p><p>皮尔逊相关系数与余弦相似度的关系为：</p><p>$$\rho(X,Y)=\frac{\mathrm{E}\left[\left(X-X_\mu\right)\left(Y-Y_\mu\right)\right]}{\sigma(X)\cdot\sigma(Y)}=\frac{\displaystyle\sum_{i=1}^{n}{\left(X-X_\mu\right)\cdot\left(Y-Y_\mu\right)}}{\left|X-X_\mu\right|\cdot\left|Y-Y_\mu\right|}=\cos{\left(X-X_\mu,Y-Y_\mu\right)}$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间皮尔逊相关系数的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>PearsonCorrelationCoefficient</span>(x, y):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(x)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(y)
</span></span><span style=display:flex><span>    x_ <span style=color:#f92672>=</span> x <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>mean(x)
</span></span><span style=display:flex><span>    y_ <span style=color:#f92672>=</span> y <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>mean(y)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>dot(x_, y_) <span style=color:#f92672>/</span> (np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(x_) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(y_))
</span></span></code></pre></div><h3 id=卡方度量chi-square-measure>卡方度量（Chi-square Measure）<a hidden class=anchor aria-hidden=true href=#卡方度量chi-square-measure>#</a></h3><p>$\mathcal{X}^2$检验通常用于检验某一观测分布是否符合典型理论分布。若观测频数与期望频数差异越小，则$\mathcal{X}^2$值越小；若观测频数与期望频数差异越大，则$\mathcal{X}^2$值越大。因此，$\mathcal{X}^2$值可以用于描述观测分布与理论分布的差异。对于$n$维空间中的任意两个分布$X$和$Y$的$\mathcal{X}^2$统计量定义为：</p><p>$$\mathcal{X}^2=\sum_{i=1}^{n}{\frac{\left(x_i-y_i\right)^2}{y_i}}=\sum_{i=1}^{k}{\frac{\left(x_i-n\cdot p_i\right)^2}{k\cdot p_i}}$$</p><p>其中，$x_i$为$X$在$i$的频数，$y_i$为$Y$在$i$的频数，$k$为总频数，$p_i$为$Y$在$i$的概率。在 <code>Python</code> 中用于计算任意两个分布间卡方度量的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ChiSquareMeasure</span>(x, y):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(x, np<span style=color:#f92672>.</span>int32)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(y, np<span style=color:#f92672>.</span>int32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>square(x <span style=color:#f92672>-</span> y) <span style=color:#f92672>/</span> y)
</span></span></code></pre></div><h3 id=交叉熵cross-entropy>交叉熵（Cross Entropy）<a hidden class=anchor aria-hidden=true href=#交叉熵cross-entropy>#</a></h3><p>交叉熵是香农信息论中的重要概念，用于度量两个分布之间的差异信息。对于$n$维空间中的任意两个分布$X$和$Y$的交叉熵定义为：</p><p>$$H(X,Y)=-\int_{p}{X(p)\cdot Y(p)\mathrm{d}p}$$</p><p>若基于分布$X$对分布$X$进行编码，其编码长度的期望为：</p><p>$$H(X)=-\int_{p}{X(p)\mathrm{d}p}$$</p><p>若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$。在 <code>Python</code> 中用于计算任意两个分布间交叉熵的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>CrossEntropy</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>sum(x <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(y))
</span></span></code></pre></div><h3 id=kl-散度kullback-leibler-divergence>KL 散度（Kullback-Leibler Divergence）<a hidden class=anchor aria-hidden=true href=#kl-散度kullback-leibler-divergence>#</a></h3><p>KL 散度又被称为相对熵（Relative Entropy）或信息散度（Information Divergence），用于度量两个分布间的差异，对于$n$维空间中的任意两个分布$X$和$Y$的 KL 散度定义为：</p><p>$$\mathrm{KL}(X|Y)=\int_{p}{X(p)\cdot\log{\frac{X(p)}{Y(p)}}\mathrm{d}p}$$</p><p>若基于分布$X$对分布$X$进行编码，其编码长度的期望为：</p><p>$$H(X)=-\int_{p}{X(p)\log{X(p)}\mathrm{d}p}$$</p><p>若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$，其多出的编码长度为：</p><p>$$\mathrm{KL}(X|Y)=H(X)-H(X,Y)$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间 KL 散度的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>KullbackLeiblerDivergence</span>(p, q):
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p)
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(q)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sum(p <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(p <span style=color:#f92672>/</span> q))
</span></span></code></pre></div><h3 id=js-散度jensen-shannon-divergence>JS 散度（Jensen-Shannon Divergence）<a hidden class=anchor aria-hidden=true href=#js-散度jensen-shannon-divergence>#</a></h3><p>JS 散度是 KL 散度的变体，解决了 KL 散度非对称的问题，对于$n$维空间中的任意两个分布$X$和$Y$的 JS 散度定义为：</p><p>$$\mathrm{JS}(X|Y)=\frac{1}{2}\cdot\mathrm{KL}(X|\frac{X+Y}{2})+\frac{1}{2}\cdot\mathrm{KL}(Y|\frac{X+Y}{2})$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间 JS 散度的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>JensenShannonDivergence</span>(p, q):
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p)
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(q)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sum(p <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> p<span style=color:#f92672>/</span>(p <span style=color:#f92672>+</span> q))) <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sum(q <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> q<span style=color:#f92672>/</span>(p <span style=color:#f92672>+</span> q)))
</span></span></code></pre></div><h3 id=海林格距离hellinger-distance>海林格距离（Hellinger Distance）<a hidden class=anchor aria-hidden=true href=#海林格距离hellinger-distance>#</a></h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的海林格距离定义为：</p><p>$$d={\frac{1}{\sqrt{2}}\cdot\sqrt{\sum_{i=1}^{n}{\left(\sqrt{x_i}-\sqrt{y_i}\right)^2}}}$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间海林格距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>HellingerDistance</span>(p, q):
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p)
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(q)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>2</span>) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(np<span style=color:#f92672>.</span>sqrt(p) <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>sqrt(q))
</span></span></code></pre></div><h3 id=α-散度α-divergence>α 散度（α Divergence）<a hidden class=anchor aria-hidden=true href=#α-散度α-divergence>#</a></h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的 α 散度被定义为：</p><p>$$d=\frac{4}{1-\alpha^2}\cdot\left[1-\int_p{X(p)^{(1+\alpha)/2}\cdot Y(p)^{(1-\alpha)/2}}\mathrm{d}x\right]$$</p><p>其中，$-\infty&lt;\alpha&lt;+\infty$为连续参数。当$\alpha\rightarrow 1$时，α 散度退化为 KL 散度；当$\alpha\rightarrow 0$时，α 散度退化为海林格距离（仅相差常系数）。</p><h3 id=f-散度f-divergence>F 散度（F Divergence）<a hidden class=anchor aria-hidden=true href=#f-散度f-divergence>#</a></h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的 F 散度被定义为：</p><p>$$\mathrm{D}(X|Y)=\int_p{Y(p)\cdot f\left[\frac{X(p)}{Y(p)}\right]\mathrm{d}p}$$</p><p>其中，函数$f(\cdot)$需满足：（1）$f(\cdot)$为凸函数；（2）$f(1)=0$。下表给出了$f(\cdot)$取不同值时，F 散度对应的结果。</p><table><thead><tr><th><strong>散度</strong></th><th>$f(\cdot)$</th></tr></thead><tbody><tr><td>卡方距离</td><td>$(t-1)^2$</td></tr><tr><td>KL 散度</td><td>$x\log{x}$</td></tr><tr><td>逆 KL 散度</td><td>$-\log{x}$</td></tr><tr><td>海林格距离</td><td>$\left(\sqrt{x}-1\right)^2$</td></tr><tr><td>α 散度</td><td>$4/(1-\alpha)^2\cdot\left(1-x^{(1+\alpha)/2}\right)$</td></tr></tbody></table><p>在 <code>Python</code> 中用于计算任意两个分布间 F 散度（海林格距离）的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(t):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> t <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>log(t)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>FDivergence</span>(p, q):
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p)
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(q)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sum(q <span style=color:#f92672>*</span> f(p <span style=color:#f92672>/</span> q))
</span></span></code></pre></div><h3 id=布雷格曼散度bregman-divergence>布雷格曼散度（Bregman Divergence）<a hidden class=anchor aria-hidden=true href=#布雷格曼散度bregman-divergence>#</a></h3><p>对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：</p><p>$$d(x,y)=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$</p><p>将上式两侧平方后，得到：</p><p>$$d^2(x,y)=\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}$$</p><p>定义$&lt;x,y>=\sum_{i=1}^{n}{x_i\cdot y_i}$，$\left|x\right|=\sqrt{&lt;x,x>}$，上式可改写为：</p><p>$$d^2(x,y)=\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}=&lt;x-y,x-y>=\left|x\right|^2-\left(\left|y\right|^2+&lt;2y,x-y>\right)$$</p><p>此处的距离即为欧几里得模函数和其在$x$处切线在$y$处的点估计差。推广该概念后，对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的布雷格曼散度定义为：</p><p>$$d(x,y)=f(x)-\left[f(y)+&lt;\nabla f(x),x-y >\right]$$</p><p>表给出了$f(\cdot)$取不同值时，布雷格曼散度对应的结果。</p><table><thead><tr><th><strong>散度</strong></th><th>$f(\cdot)$</th></tr></thead><tbody><tr><td>平方损失</td><td>$x^2$</td></tr><tr><td>/</td><td>$x\log{x}$</td></tr><tr><td>Logistic 损失</td><td>$x\log{x}+(1-x)\log{(1-x)}$</td></tr><tr><td>Itakura-Saito 距离</td><td>$-\log{x}$</td></tr><tr><td>/</td><td>$e^x$</td></tr><tr><td>平方欧几里得距离</td><td>$\left|x\right|^2$</td></tr><tr><td>马氏距离</td><td>$\mathbf{X}^TA\mathbf{X}$</td></tr><tr><td>KL 散度</td><td>$\sum_{i=1}^{n}{x_i\log{x_i}}$</td></tr></tbody></table><h3 id=wasserstein-距离wasserstein-distance>Wasserstein 距离（Wasserstein Distance）<a hidden class=anchor aria-hidden=true href=#wasserstein-距离wasserstein-distance>#</a></h3><p>Wasserstein 距离被称为推土机距离，用于表示两个分布的相似度。Wasserstein 距离定义为把分布$X$转变成分布$Y$所需移动的平均距离的最小值，如下图所示：</p><figure><div class=fancybox><a data-fancybox=gallery data-src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/08d304f02c41c5db0b68d947d1c1c805.png data-caption="图 5：Wasserstein 距离（Wasserstein Distance）" href=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/08d304f02c41c5db0b68d947d1c1c805.png><img loading=lazy src=https://cos.ap-shanghai.myqcloud.com/blog-1307064178/contents/2023/08d304f02c41c5db0b68d947d1c1c805.png alt="图 5：Wasserstein 距离（Wasserstein Distance）"></a></div><figcaption><p>图 5：Wasserstein 距离（Wasserstein Distance）</p></figcaption></figure><p>对于$n$维空间中的任意两个分布$X$和$Y$的 Wasserstein 距离定义为：</p><p>$$d=\inf_{\gamma\sim\Pi(X,Y)}{\mathrm{E}_{p,q\sim\gamma}\left|p-q\right|}$$</p><p>其中，$\Pi(X,Y)$为分布$X$和分布$Y$构成的联合分布的集合，$\gamma$为$\Pi(X,Y)$中任意分布，$p$和$q$是分布$\gamma$中的样本。</p><h3 id=巴氏距离bhattacharyya-distance>巴氏距离（Bhattacharyya Distance）<a hidden class=anchor aria-hidden=true href=#巴氏距离bhattacharyya-distance>#</a></h3><p>巴氏系数可以用来度量两个分布的相似性，对于$n$维空间中的任意两个分布$X$和$Y$的巴氏系数定义为：</p><p>$$c_b=\int_{p}{\sqrt{X(p)\cdot Y(p)}\mathrm{d}p}$$</p><p>巴氏距离定义为：</p><p>$$d_b=-\ln{c_b}$$</p><p>需要注意的是，海林格距离$d=\sqrt{1-d_b}$。在 <code>Python</code> 中用于计算任意两个分布间巴氏距离的代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>BhattacharyyaCoefficient</span>(p,q):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(p)
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(q)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>sqrt(p <span style=color:#f92672>*</span> q))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>HellingerDistance</span>(p, q):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> BhattacharyyaCoefficient(p, q))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>BhattacharyyaDistance</span>(p, q):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>log(BhattacharyyaCoefficient(p, q))
</span></span></code></pre></div><h3 id=最大均值差异maximum-mean-discrepancy>最大均值差异（Maximum Mean Discrepancy）<a hidden class=anchor aria-hidden=true href=#最大均值差异maximum-mean-discrepancy>#</a></h3><p>最大均值差异是迁移学习领域中最为广泛使用的一种损失函数，它度量了再生希尔伯特空间中两个不同分布间的距离。通过在样本空间寻找连续函数$f:X\rightarrow R$随机投影后，分别求这两个分布在$f$上函数值的均值，并通过做差得到均值差异（Mean Discrepancy）。最大化均值差异即为寻找一个$f$使得均值差异最大。对于$n$维空间中的任意两个分布$X$和$Y$的最大均值差异定义为：</p><p>$$d=\sup_{|f|_\mathrm{H} \leq 1}{\mathrm{E}_p[f(X)]-\mathrm{E}_q[f(Y)]}$$</p><h3 id=点间互信息pointwise-mutual-information>点间互信息（Pointwise Mutual Information）<a hidden class=anchor aria-hidden=true href=#点间互信息pointwise-mutual-information>#</a></h3><p>点间互信息用来衡量两个分布的相关性，对于$n$维空间中的任意两个分布$X$和$Y$的点间互信息定义为：</p><p>$$d=\log{\frac{p(X,Y)}{p(X)\cdot p(Y)}}=\log{\frac{p(X|Y)}{p(X)}}=\log{\frac{p(Y|X)}{p(Y)}}$$</p><p>若$X$和$Y$不相关，则$P(X,Y)=P(X)\cdot P(Y)$。</p></div><footer class=post-footer><div class=copyright-box style="margin:0 auto;margin-bottom:15px;text-align:center;border:1px dashed #ccc;padding:10px"><p>本作品采用<a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel="noopener noreferrer" style="box-shadow:0 1px">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。</p><div style=margin-top:5px><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel="noopener noreferrer"><img src=https://kwang.life/images/copyright/CC_BY-NC-SA.svg alt=CC-BY-NC-SA style=width:auto;height:42px;display:inline-block></a>
<a href=https://notbyai.fyi target=_blank rel="noopener noreferrer"><img src=https://kwang.life/images/copyright/Written-By-Human-Not-By-AI-Badge-white.svg alt=真人撰写，非AI生成 style=width:auto;height:42px;display:inline-block></a></div></div><ul class=post-tags><li><a href=https://kwang.life/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://kwang.life/tags/%E6%B5%8B%E5%BA%A6%E8%AE%BA/>测度论</a></li><li><a href=https://kwang.life/tags/%E7%A9%BA%E9%97%B4%E8%B7%9D%E7%A6%BB/>空间距离</a></li><li><a href=https://kwang.life/tags/%E5%AD%97%E7%AC%A6%E8%B7%9D%E7%A6%BB/>字符距离</a></li><li><a href=https://kwang.life/tags/%E9%9B%86%E5%90%88%E8%B7%9D%E7%A6%BB/>集合距离</a></li><li><a href=https://kwang.life/tags/%E5%88%86%E5%B8%83%E8%B7%9D%E7%A6%BB/>分布距离</a></li></ul><nav class=paginav><a class=prev href=https://kwang.life/2023/08/%E4%BF%AE%E6%98%94%E5%BA%95%E5%BE%B7%E9%99%B7%E9%98%B1thucydides-trap/><span class=title>« 上一页</span><br><span>修昔底德陷阱（Thucydides' Trap）</span></a>
<a class=next href=https://kwang.life/2022/12/%E9%9B%86%E6%88%90%E7%94%B5%E8%B7%AF%E7%89%88%E5%9B%BE%E8%AE%BE%E8%AE%A1%E5%AE%9E%E9%AA%8C%E4%BB%A5%E8%BF%90%E7%AE%97%E6%94%BE%E5%A4%A7%E5%99%A8%E5%92%8C%E5%B8%A6%E9%9A%99%E5%9F%BA%E5%87%86%E6%BA%90%E4%B8%BA%E4%BE%8B/><span class=title>下一页 »</span><br><span>集成电路版图设计实验（以运算放大器和带隙基准源为例）</span></a></nav></footer><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.17/twikoo.all.min.js></script>
<script>twikoo.init({envId:'https://kwang-twikoo.zeabur.app/',el:'#tcomment'})</script></article></main><footer class=footer><span>&copy; 2023 <a href=https://kwang.life>退思轩</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><br><div class=busuanzi-footer><span id=busuanzi_container_site_pv>总访问量&nbsp<span id=busuanzi_value_site_pv></span>&nbsp次
       </span>
&nbsp;·&nbsp;
<span id=busuanzi_container_site_uv>总访客数&nbsp<span id=busuanzi_value_site_uv></span>&nbsp位
       </span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement('button');e.classList.add('copy-code'),e.innerHTML='复制';function s(){e.innerHTML='已复制！',setTimeout(()=>{e.innerHTML='复制'},2e3)}e.addEventListener('click',o=>{if('clipboard'in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand('copy'),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>机器学习中常用的距离度量汇总 | 退思轩</title><meta name=keywords content="神经网络,测度论,空间距离,字符距离,集合距离,分布距离"><meta name=description content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质：
非负性：$d(x,y)\geq 0$ 同一性：$d(x,y)=0\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。
空间距离 欧几里得距离（Euclidean Distance） 欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\mathcal{L}_2$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：
$$d=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$
在 Python 中用于计算任意两点间欧几里得距离的代码如下：
import numpy as np def EuclideanDistance(x, y): x = np.array(x) y = np.array(y) return np.sqrt(np.sum(np.square(x - y))) 曼哈顿距离（Manhattan Distance） 曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\mathcal{L}_1$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的曼哈顿距离定义为：
$$d=\sqrt{\sum_{i=1}^{n}{\left|x_i-y_i\right|}}$$
下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。
曼哈顿距离（Manhattan Distance）
在 Python 中用于计算任意两点间曼哈顿距离的代码如下：
import numpy as np def ManhattanDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) 切比雪夫距离（Chebyschev Distance） 切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\mathcal{L}_\infty$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的切比雪夫距离定义为：
$$d=\max\left|x_i-y_i\right|$$
以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。
切比雪夫距离（Chebyschev Distance）"><meta name=author content="Kai Wang"><link rel=canonical href=https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/><meta name=google-site-verification content="G-FVKQ0YJ3T3"><link crossorigin=anonymous href=/assets/css/stylesheet.dd45d6d80113600bd6801743341866cafc6f13851156230d97c75511dd86d561.css integrity="sha256-3UXW2AETYAvWgBdDNBhmyvxvE4URViMNl8dVEd2G1WE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kwang.life/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kwang.life/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kwang.life/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kwang.life/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kwang.life/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-FVKQ0YJ3T3"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-FVKQ0YJ3T3',{anonymize_ip:!1})}</script><meta property="og:title" content="机器学习中常用的距离度量汇总"><meta property="og:description" content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质：
非负性：$d(x,y)\geq 0$ 同一性：$d(x,y)=0\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。
空间距离 欧几里得距离（Euclidean Distance） 欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\mathcal{L}_2$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：
$$d=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$
在 Python 中用于计算任意两点间欧几里得距离的代码如下：
import numpy as np def EuclideanDistance(x, y): x = np.array(x) y = np.array(y) return np.sqrt(np.sum(np.square(x - y))) 曼哈顿距离（Manhattan Distance） 曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\mathcal{L}_1$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的曼哈顿距离定义为：
$$d=\sqrt{\sum_{i=1}^{n}{\left|x_i-y_i\right|}}$$
下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。
曼哈顿距离（Manhattan Distance）
在 Python 中用于计算任意两点间曼哈顿距离的代码如下：
import numpy as np def ManhattanDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) 切比雪夫距离（Chebyschev Distance） 切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\mathcal{L}_\infty$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的切比雪夫距离定义为：
$$d=\max\left|x_i-y_i\right|$$
以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。
切比雪夫距离（Chebyschev Distance）"><meta property="og:type" content="article"><meta property="og:url" content="https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/"><meta property="og:image" content="https://kwang.life/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-18T11:27:00+08:00"><meta property="article:modified_time" content="2023-08-18T11:27:00+08:00"><meta property="og:site_name" content="退思轩"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kwang.life/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="机器学习中常用的距离度量汇总"><meta name=twitter:description content="距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质：
非负性：$d(x,y)\geq 0$ 同一性：$d(x,y)=0\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。
空间距离 欧几里得距离（Euclidean Distance） 欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\mathcal{L}_2$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：
$$d=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$
在 Python 中用于计算任意两点间欧几里得距离的代码如下：
import numpy as np def EuclideanDistance(x, y): x = np.array(x) y = np.array(y) return np.sqrt(np.sum(np.square(x - y))) 曼哈顿距离（Manhattan Distance） 曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\mathcal{L}_1$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的曼哈顿距离定义为：
$$d=\sqrt{\sum_{i=1}^{n}{\left|x_i-y_i\right|}}$$
下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。
曼哈顿距离（Manhattan Distance）
在 Python 中用于计算任意两点间曼哈顿距离的代码如下：
import numpy as np def ManhattanDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) 切比雪夫距离（Chebyschev Distance） 切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\mathcal{L}_\infty$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的切比雪夫距离定义为：
$$d=\max\left|x_i-y_i\right|$$
以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。
切比雪夫距离（Chebyschev Distance）"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://kwang.life/posts/"},{"@type":"ListItem","position":3,"name":"机器学习中常用的距离度量汇总","item":"https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"机器学习中常用的距离度量汇总","name":"机器学习中常用的距离度量汇总","description":"距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质：\n非负性：$d(x,y)\\geq 0$ 同一性：$d(x,y)=0\\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。\n空间距离 欧几里得距离（Euclidean Distance） 欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\\mathcal{L}_2$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的欧几里得距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}}$$\n在 Python 中用于计算任意两点间欧几里得距离的代码如下：\nimport numpy as np def EuclideanDistance(x, y): x = np.array(x) y = np.array(y) return np.sqrt(np.sum(np.square(x - y))) 曼哈顿距离（Manhattan Distance） 曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\\mathcal{L}_1$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的曼哈顿距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left|x_i-y_i\\right|}}$$\n下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。\n曼哈顿距离（Manhattan Distance）\n在 Python 中用于计算任意两点间曼哈顿距离的代码如下：\nimport numpy as np def ManhattanDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) 切比雪夫距离（Chebyschev Distance） 切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\\mathcal{L}_\\infty$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的切比雪夫距离定义为：\n$$d=\\max\\left|x_i-y_i\\right|$$\n以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。\n切比雪夫距离（Chebyschev Distance）","keywords":["神经网络","测度论","空间距离","字符距离","集合距离","分布距离"],"articleBody":"距离的定义 在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于距离度量函数$d(x,y)$，其需要满足如下性质：\n非负性：$d(x,y)\\geq 0$ 同一性：$d(x,y)=0\\Leftrightarrow x=y$ 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\\leq d(x,z)+d(z,y)$ 根据样本特征空间的不同，我们把度量的距离分为：空间距离、字符距离、集合距离、分布距离。\n空间距离 欧几里得距离（Euclidean Distance） 欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\\mathcal{L}_2$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的欧几里得距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}}$$\n在 Python 中用于计算任意两点间欧几里得距离的代码如下：\nimport numpy as np def EuclideanDistance(x, y): x = np.array(x) y = np.array(y) return np.sqrt(np.sum(np.square(x - y))) 曼哈顿距离（Manhattan Distance） 曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\\mathcal{L}_1$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的曼哈顿距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left|x_i-y_i\\right|}}$$\n下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。\n曼哈顿距离（Manhattan Distance）\n在 Python 中用于计算任意两点间曼哈顿距离的代码如下：\nimport numpy as np def ManhattanDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) 切比雪夫距离（Chebyschev Distance） 切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\\mathcal{L}_\\infty$范数。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的切比雪夫距离定义为：\n$$d=\\max\\left|x_i-y_i\\right|$$\n以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。\n切比雪夫距离（Chebyschev Distance）\n在 Python 中用于计算任意两点间切比雪夫距离的代码如下：\nimport numpy as np def ChebyshevDistance(x, y): x = np.array(x) y = np.array(y) return np.max(np.abs(x - y)) 闵可夫斯基距离（Minkowski Distance） 闵可夫斯基距离并非一种新型的距离度量方式，而是一种对于多种不同距离度量的概括性表述。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的闵可夫斯基距离定义为：\n$$d=\\left(\\sum_{i=1}^{n}{\\left|x_i-y_i\\right|^p}\\right)^{1/p}$$\n其中，$p$为闵可夫斯基距离参数。当$p=1$时，闵可夫斯基距离退化为曼哈顿距离；当$p=2$时，闵可夫斯基距离退化为欧几里得距离；当$p\\rightarrow\\infty$时，闵可夫斯基距离退化为切比雪夫距离，如下图所示：\n闵可夫斯基距离（Minkowski Distance）\n在 Python 中用于计算任意两点间闵可夫斯基距离的代码如下：\nimport math import numpy as np def MinkowskiDistance(x, y, p): zipped_coordinate = zip(x, y) return math.pow(np.sum(math.pow(np.abs(x - y), p)), 1 / p) 标准化欧几里得距离（Standardized Euclidean Distance） 标准化欧几里得距离是将欧式空间中任意两点的分量都“标准化”到均值、方差一致的区间，记每个分量的均值为$\\mu$，方差为$\\sigma_i^2$，“标准化”结果为$X^*$：\n$$X^*=\\frac{X-\\mu}{\\sigma^2}$$\n对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的标准化欧几里得距离定义为：\n$$d=\\sqrt{\\sum_{i=1}^{n}{\\left(\\frac{x_i-y_i}{\\sigma_i^2}\\right)^2}}$$\n如果将$1/\\sigma^2$看做权重，则标准化欧几里得距离可以被认为是加权欧几里得距离。\n在 Python 中用于计算任意两点间标准化欧几里得距离的代码如下（需要注意 sigma ≠ 0）：\nimport numpy as np def StandardizedEuclideanDistance(x, y): x = np.array(x) y = np.array(y) X = np.vstack([x,y]) sigma = np.var(X, axis=0, ddof=1) return np.sqrt(((x - y) ** 2 /sigma).sum()) 马氏距离（Mahalanobis Distance） 马氏距离由印度统计学家马哈拉诺比斯（P. C. Mahalanobis）提出，用于表示数据的协方差距离，它可以有效地表示两个未知样本集之间的相似度。对于$n$维空间中的任意两个样本集$\\textbf{X}$和$\\textbf{Y}$的马氏距离定义为：\n$$d=\\sqrt{\\left(\\textbf{X}-\\textbf{Y}\\right)^T\\Sigma^{-1}\\left(\\textbf{X}-\\textbf{Y}\\right)}$$\n其中，$\\Sigma$为样本集$X$和$Y$的协方差矩阵。若协方差矩阵为单位矩阵，则马氏距离退化为欧几里得距离；若协方差矩阵为对角矩阵，则马氏距离退化为标准化欧几里得距离。\n需要注意的是，马氏距离的计算需要确保$\\Sigma$的逆矩阵存在，否则可以直接采用欧几里得距离进行计算。此外，马氏距离不受量纲的影响，它可以排除变量之间相关性的干扰，但同时也夸大了微小变化量的作用。例如，若将两个相同的样本放入两个不同的总体中，经计算的到的马氏距离也是不同的（除非它们的$\\Sigma$恰巧相同）。\n在 Python 中用于计算任意两个样本集间马氏距离的代码如下：\nimport numpy as np def MahalanobisDistance(x, y): x = np.array(x) y = np.array(y) X = np.vstack([x,y]) X_T = X.T sigma_inverse = np.linalg.inv(np.cov(X)) d = [] for i in range(0, X_T.shape[0]): for j in range(i + 1, X_T.shape[0]): delta = X_T[i] - X_T[j] d.append(np.sqrt(np.dot(np.dot(delta,sigma_inverse),delta.T))) return d 兰氏距离（Lance and Williams Distance） 兰氏距离又被称为堪培拉距离（Canberra Distance），可以理解为加权曼哈顿距离。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的兰氏距离定义为：\n$$d=\\sum_{i=1}^{n}{\\frac{\\left|x_i-y_i\\right|}{\\left|x_i\\right|+\\left|y_i\\right|}}$$\n在 Python 中用于计算任意两点间兰氏距离的代码如下：\nimport numpy as np def CanberraDistance(x, y): x = np.array(x) y = np.array(y) d = 0 for i in range(len(x)): if x[i] == 0 and y[i] == 0: d += 0 else: d += abs(x[i] - y[i]) / (abs(x[i]) + abs(y[i])) return d 余弦相似度（Cosine Similarity） 在几何学中，通常采用余弦相似度来度量两个向量间的夹角，其取值为$[-1,1]$。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的余弦相似度定义为：\n$$\\cos{\\left(\\vec{x},\\vec{y}\\right)}=\\frac{\\vec{x}\\cdot\\vec{y}}{\\left|\\vec{x}\\right|\\cdot\\left|\\vec{y}\\right|}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i\\cdot y_i}}{\\sqrt{\\displaystyle\\sum_{i=1}^{n}{x_i^2}}\\cdot\\sqrt{\\displaystyle\\sum_{i=1}^{n}{y_i^2}}}$$\n当$\\cos{\\left(\\vec{x},\\vec{y}\\right)}\\rightarrow0$时，两向量完全正交；当$\\cos{\\left(\\vec{x},\\vec{y}\\right)}\\rightarrow1$时，两向量完全重合；当$\\cos{\\left(\\vec{x},\\vec{y}\\right)}\\rightarrow -1$时，两向量完全相反。在 Python 中用于计算任意两点间余弦相似度的代码如下：\nimport numpy as np def CosineDistance(x, y): x = np.array(x) y = np.array(y) return np.dot(x,y) / (np.linalg.norm(x) * np.linalg.norm(y)) 测地距离（Geodesic Distance） 测地距离原指球体表面上两点间的最短距离，后来被推广到其它领域。在图论中，测地距离为两顶点间的最短路径；在欧式空间中，测地距离为欧几里得距离；在非欧空间中，测地距离为连接两点间的最短圆弧。如下图所示：\n测地距离（Geodesic Distance）\n布雷柯蒂斯距离（Bray Curtis Distance） 布雷柯蒂斯距离主要用于生态学和环境科学领域，用于计算不同样本间的差异。对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的布雷柯蒂斯距离定义为：\n$$d=\\frac{\\displaystyle \\sum_{i=1}^{n}{\\left|x_i-y_i\\right|}}{\\displaystyle \\sum_{i=1}^{n}{x_i}+\\sum_{i=1}^{n}{y_i}}$$\n在 Python 中用于计算任意两点间布雷柯蒂斯距离的代码如下：\nimport numpy as np def BrayCurtisDistance(x, y): x = np.array(x) y = np.array(y) return np.sum(np.abs(x - y)) / (np.sum(x) + np.sum(y)) 半正矢距离（Haversine Distance） 半正矢距离用于计算任意两经纬点间的距离，对于空间中的任意两经纬点$A(\\mathrm{lon}_1,\\mathrm{lat}_1)$和$B(\\mathrm{lon}_2,\\mathrm{lat}_2)$的半正矢距离定义为：\n$$d=2r\\cdot\\arcsin{\\sqrt{\\sin^2{\\frac{\\mathrm{lat}_2-\\mathrm{lat}_1}{2}}+\\cos{(\\mathrm{lat}_1)}\\cos{(\\mathrm{lat}_2)}\\sin^2{\\frac{\\mathrm{lon}_2-\\mathrm{lon}_1}{2}}}}$$\n其中，$r$为半径。在 Python 中用于计算任意两经纬点间半正矢距离的代码如下：\nimport numpy as np def HaversineDistance(lon1, lat1, lon2, lat2): lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) c = 2 * 6367 * 1000 *np.arcsin(np.sqrt(np.sin((lat2 - lat1) / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin((lon2 - lon1) / 2.0) ** 2)) 字符距离 汉明距离（Hamming Distance） 汉明距离由两个字符串对应位不同的数量决定，通常用于数据传输差错控制编码领域。对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的汉明距离定义为：\n$$d=\\sum_{i=0}^{n}{x_i\\otimes y_i}$$\n汉明距离也可以理解为将$A$变为$B$的最小操作次数。在 Python 中用于计算任意两个字符串间汉明距离的代码如下：\ndef HammingDistance(x, y): return sum(x_ch != y_ch for x_ch, y_ch in zip(x, y)) 莱文斯坦距离（Levenshtein Distance） 莱文斯坦距离又被称为编辑距离（Edit Distance），用于度量两个字符串之间的差异，定义为：将字符串$A$转化为字符串$B$所需的最少单字符编辑（插入、删除或替换）次数。对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$，$A$的前$i$个字符和$B$的前$j$个字符的莱文斯坦距离定义为：\n$$d(i,j) = \\begin{align*} \\left{\\begin{matrix} \\max{\\left(i,j\\right)}, \u0026 \\min{\\left(i,j\\right)}=0 \\ \\min{\\left[d(i-1,j),d(i,j-1), d(i,j)\\right]} + I(i,j), \u0026 \\min{\\left(i,j\\right)\\neq 0} \\end{matrix}\\right. \\end{align*}$$\n其中，$I(\\cdot)$为指示函数，当$x_i=y_j$时，$I(i,j)=0$；当$x_i\\neq y_j$时，$I(i,j)=1$。在 Python 中用于计算任意两个字符串间莱文斯坦距离的代码如下：\ndef LevenshteinDistance(x, y): dp = np.zeros((len(x) + 1,len(y) + 1)) for i in range(len(x) + 1): dp[i][0] = i for j in range(len(y) + 1): dp[0][j] = j for i in range(1, len(x) + 1): for j in range(1, len(y) + 1): delta = 0 if x[i-1] == y[j-1] else 1 dp[i][j] = min(dp[i - 1][j - 1] + delta, min(dp[i-1][j] + 1, dp[i][j - 1] + 1)) return int(dp[len(x)][len(y)]) 归一化 Google 距离（Normalized Google Distance） 归一化 Google 距离是由给定一组关键词集合的 Google 搜索引擎所返回的命中数量决定的，它是一种语义相似度量方法。对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的归一化 Google 距离定义为：\n$$d=\\frac{\\max{\\left[\\log{f(A)},\\log{f(B)}\\right]}-\\log{f(A,B)}}{\\log{M}-\\min{\\left[\\log{f(A)}, \\log{f(B)}\\right]}}$$\n其中，$M$为 Google 搜索引擎所返回的网页总数；$f(x)$和$f(y)$分别为 Google 搜索引擎返回关于字符串$A$和字符串$B$的命中数量；$f(A,B)$为 Google 搜索引擎返回关于字符串$A$和字符串$B$同时出现的命中数量。\nJaro-Winkler 相似度（Jaro-Winkler Similarity） Jaro 相似度用于评估两个字符串间的相似度，对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的 Jaro 相似度定义为：\n$$d_j=\\frac{1}{3}\\cdot\\left(\\frac{m}{|A|}+\\frac{m}{|B|}+\\frac{m-t}{m}\\right)$$\n其中，$m$是匹配的字符数，$t$是替换的字符数。\nJaro-Winkler 相似度在 Jaro 相似度的基础上引入了前缀的概念，对于长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的 Jaro-Winkler 相似度定义为：\n$$d_w=d_j+l\\cdot p\\cdot(1-d_j)$$\n其中，$l$为字符串$A$和字符串$B$的共同前缀的字符数；$p$为缩放因子（常取$p=0.1$）。\n李距离（Lee Distance） 李距离是编码理论中用于描述字符串距离的方案，对于使用包含$q$个字母的字母表且长度为$n$的任意两个字符串$A=x_1x_2\\cdots x_n$和$B=y_1y_2\\cdots y_n$的李距离定义为：\n$$d=\\sum_{i=0}^{n}{\\min{\\left(\\left|x_i-y_i\\right|,q-\\left|x_i-y_i\\right|\\right)}}$$\n当$q=2$或$q=3$时，李距离退化为汉明距离。\n集合距离 杰卡德相似系数（Jaccard Similarity Coefficient） 杰卡德相似系数是指两个集合$A$和$B$中相同元素在所有元素中的占比，定义为$J(A,B)$：\n$$J(A,B)=\\frac{\\left|A\\cap B\\right|}{\\left|A\\cup B\\right|}$$\n杰卡德距离（Jaccard Distance）是指两个集合$A$和$B$中不同元素在所有元素中的占比，定义为$J_\\delta(A,B)$：\n$$J_\\delta(A,B)=1-J(A,B)=1-\\frac{\\left|A\\cap B\\right|}{\\left|A\\cup B\\right|}$$\n在 Python 中用于计算任意两个集合间杰卡德相似系数的代码如下：\nimport numpy as np def JaccardSimilarityCoefficient(x, y): x = np.asarray(x, np.int32) y = np.asarray(y, np.int32) return np.double(np.bitwise_and((x != y), np.bitwise_or(x != 0, y != 0)).sum()) / np.double(np.bitwise_or(x != 0, y != 0).sum()) 奥奇亚系数（Ochiia Coefficient） 奥奇亚系数是指两个集合$A$和$B$中相同元素与两集合大小几何平均值的比值，定义为$K(A,B)$：\n$$K(A,B)=\\frac{\\left|A\\cap B\\right|}{\\sqrt{\\left|A\\right|\\times\\left|B\\right|}}$$\n戴斯系数（Dice Coefficient） 戴斯系数除了可以用来衡量两个集合间的距离，还可以用来衡量两个字符串间的距离，定义为$D(A,B)$：\n$$D(A,B)=\\frac{2\\left|A\\cap B\\right|}{\\left|A\\right|+\\left|B\\right|}$$\n豪斯多夫距离（Hausdorff Distance） 豪斯多夫距离用于度量两个集合$A$和$B$间的距离，定义为$H(A,B)$：\n$$H(A,B)=\\max{\\left[h(A,B),h(B,A)\\right]}$$\n其中，$h(\\cdot)$为双向豪斯多夫距离，定义为：\n$$h(A,B)=\\max_{a\\in A}{\\min_{b\\in B}{\\left |a-b\\right|}}$$\n其中，$h(A,B)$为从集合$A$到集合$B$的单向豪斯多夫距离，$h(B,A)$为从集合$B$到集合$A$的单向豪斯多夫距离。\n分布距离 皮尔逊相关系数（Pearson Correlation Coefficient） 皮尔逊相关系数又被称为皮尔逊积矩相关技术（Pearson Product Moment Correlation Coefficient，PPMCC/PCC），用于度量两个变量$X$和$Y$之间的线性相关性。与上文提到的余弦相似度不同，皮尔逊相关系数不受平移变换的影响。对于$n$维空间中的任意两个分布$X$和$Y$的皮尔逊相关系数定义为：\n$$\\rho(X,Y)=\\frac{\\mathrm{cov}(X,Y)}{\\sigma(X)\\cdot\\sigma(Y)}=\\frac{\\mathrm{E}\\left[\\left(X-X_\\mu\\right)\\left(Y-Y_\\mu\\right)\\right]}{\\sigma(X)\\cdot\\sigma(Y)}$$\n皮尔逊相关系数与余弦相似度的关系为：\n$$\\rho(X,Y)=\\frac{\\mathrm{E}\\left[\\left(X-X_\\mu\\right)\\left(Y-Y_\\mu\\right)\\right]}{\\sigma(X)\\cdot\\sigma(Y)}=\\frac{\\displaystyle\\sum_{i=1}^{n}{\\left(X-X_\\mu\\right)\\cdot\\left(Y-Y_\\mu\\right)}}{\\left|X-X_\\mu\\right|\\cdot\\left|Y-Y_\\mu\\right|}=\\cos{\\left(X-X_\\mu,Y-Y_\\mu\\right)}$$\n在 Python 中用于计算任意两个分布间皮尔逊相关系数的代码如下：\nimport numpy as np def PearsonCorrelationCoefficient(x, y): x = np.array(x) y = np.array(y) x_ = x - np.mean(x) y_ = y - np.mean(y) return np.dot(x_, y_) / (np.linalg.norm(x_) * np.linalg.norm(y_)) 卡方度量（Chi-square Measure） $\\mathcal{X}^2$检验通常用于检验某一观测分布是否符合典型理论分布。若观测频数与期望频数差异越小，则$\\mathcal{X}^2$值越小；若观测频数与期望频数差异越大，则$\\mathcal{X}^2$值越大。因此，$\\mathcal{X}^2$值可以用于描述观测分布与理论分布的差异。对于$n$维空间中的任意两个分布$X$和$Y$的$\\mathcal{X}^2$统计量定义为：\n$$\\mathcal{X}^2=\\sum_{i=1}^{n}{\\frac{\\left(x_i-y_i\\right)^2}{y_i}}=\\sum_{i=1}^{k}{\\frac{\\left(x_i-n\\cdot p_i\\right)^2}{k\\cdot p_i}}$$\n其中，$x_i$为$X$在$i$的频数，$y_i$为$Y$在$i$的频数，$k$为总频数，$p_i$为$Y$在$i$的概率。在 Python 中用于计算任意两个分布间卡方度量的代码如下：\nimport numpy as np def ChiSquareMeasure(x, y): x = np.asarray(x, np.int32) y = np.asarray(y, np.int32) return np.sum(np.square(x - y) / y) 交叉熵（Cross Entropy） 交叉熵是香农信息论中的重要概念，用于度量两个分布之间的差异信息。对于$n$维空间中的任意两个分布$X$和$Y$的交叉熵定义为：\n$$H(X,Y)=-\\int_{p}{X(p)\\cdot Y(p)\\mathrm{d}p}$$\n若基于分布$X$对分布$X$进行编码，其编码长度的期望为：\n$$H(X)=-\\int_{p}{X(p)\\mathrm{d}p}$$\n若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$。在 Python 中用于计算任意两个分布间交叉熵的代码如下：\nimport numpy as np def CrossEntropy(x, y): return -np.sum(x * np.log(y)) KL 散度（Kullback-Leibler Divergence） KL 散度又被称为相对熵（Relative Entropy）或信息散度（Information Divergence），用于度量两个分布间的差异，对于$n$维空间中的任意两个分布$X$和$Y$的 KL 散度定义为：\n$$\\mathrm{KL}(X|Y)=\\int_{p}{X(p)\\cdot\\log{\\frac{X(p)}{Y(p)}}\\mathrm{d}p}$$\n若基于分布$X$对分布$X$进行编码，其编码长度的期望为：\n$$H(X)=-\\int_{p}{X(p)\\log{X(p)}\\mathrm{d}p}$$\n若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$，其多出的编码长度为：\n$$\\mathrm{KL}(X|Y)=H(X)-H(X,Y)$$\n在 Python 中用于计算任意两个分布间 KL 散度的代码如下：\nimport numpy as np def KullbackLeiblerDivergence(p, q): p = np.array(p) q = np.array(q) return np.sum(p * np.log(p / q)) JS 散度（Jensen-Shannon Divergence） JS 散度是 KL 散度的变体，解决了 KL 散度非对称的问题，对于$n$维空间中的任意两个分布$X$和$Y$的 JS 散度定义为：\n$$\\mathrm{JS}(X|Y)=\\frac{1}{2}\\cdot\\mathrm{KL}(X|\\frac{X+Y}{2})+\\frac{1}{2}\\cdot\\mathrm{KL}(Y|\\frac{X+Y}{2})$$\n在 Python 中用于计算任意两个分布间 JS 散度的代码如下：\nimport numpy as np def JensenShannonDivergence(p, q): p = np.array(p) q = np.array(q) return 0.5 * np.sum(p * np.log(2 * p/(p + q))) + 0.5 * np.sum(q * np.log(2 * q/(p + q))) 海林格距离（Hellinger Distance） 对于$n$维空间中的任意两个分布$X$和$Y$的海林格距离定义为：\n$$d={\\frac{1}{\\sqrt{2}}\\cdot\\sqrt{\\sum_{i=1}^{n}{\\left(\\sqrt{x_i}-\\sqrt{y_i}\\right)^2}}}$$\n在 Python 中用于计算任意两个分布间海林格距离的代码如下：\nimport numpy as np def HellingerDistance(p, q): p = np.array(p) q = np.array(q) return 1 / np.sqrt(2) * np.linalg.norm(np.sqrt(p) - np.sqrt(q)) α 散度（α Divergence） 对于$n$维空间中的任意两个分布$X$和$Y$的 α 散度被定义为：\n$$d=\\frac{4}{1-\\alpha^2}\\cdot\\left[1-\\int_p{X(p)^{(1+\\alpha)/2}\\cdot Y(p)^{(1-\\alpha)/2}}\\mathrm{d}x\\right]$$\n其中，$-\\infty\u003c\\alpha\u003c+\\infty$为连续参数。当$\\alpha\\rightarrow 1$时，α 散度退化为 KL 散度；当$\\alpha\\rightarrow 0$时，α 散度退化为海林格距离（仅相差常系数）。\nF 散度（F Divergence） 对于$n$维空间中的任意两个分布$X$和$Y$的 F 散度被定义为：\n$$\\mathrm{D}(X|Y)=\\int_p{Y(p)\\cdot f\\left[\\frac{X(p)}{Y(p)}\\right]\\mathrm{d}p}$$\n其中，函数$f(\\cdot)$需满足：（1）$f(\\cdot)$为凸函数；（2）$f(1)=0$。下表给出了$f(\\cdot)$取不同值时，F 散度对应的结果。\n散度 $f(\\cdot)$ 卡方距离 $(t-1)^2$ KL 散度 $x\\log{x}$ 逆 KL 散度 $-\\log{x}$ 海林格距离 $\\left(\\sqrt{x}-1\\right)^2$ α 散度 $4/(1-\\alpha)^2\\cdot\\left(1-x^{(1+\\alpha)/2}\\right)$ 在 Python 中用于计算任意两个分布间 F 散度（海林格距离）的代码如下：\nimport numpy as np def f(t): return t * np.log(t) def FDivergence(p, q): p = np.array(p) q = np.array(q) return np.sum(q * f(p / q)) 布雷格曼散度（Bregman Divergence） 对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的欧几里得距离定义为：\n$$d(x,y)=\\sqrt{\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}}$$\n将上式两侧平方后，得到：\n$$d^2(x,y)=\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}$$\n定义$=\\sum_{i=1}^{n}{x_i\\cdot y_i}$，$\\left|x\\right|=\\sqrt{}$，上式可改写为：\n$$d^2(x,y)=\\sum_{i=1}^{n}{\\left(x_i-y_i\\right)^2}==\\left|x\\right|^2-\\left(\\left|y\\right|^2+\u003c2y,x-y\u003e\\right)$$\n此处的距离即为欧几里得模函数和其在$x$处切线在$y$处的点估计差。推广该概念后，对于$n$维空间中的任意两点$A(x_1,x_2,\\cdots,x_n)$和$B(y_1,y_2,\\cdots,y_n)$的布雷格曼散度定义为：\n$$d(x,y)=f(x)-\\left[f(y)+\u003c\\nabla f(x),x-y \u003e\\right]$$\n表给出了$f(\\cdot)$取不同值时，布雷格曼散度对应的结果。\n散度 $f(\\cdot)$ 平方损失 $x^2$ / $x\\log{x}$ Logistic 损失 $x\\log{x}+(1-x)\\log{(1-x)}$ Itakura-Saito 距离 $-\\log{x}$ / $e^x$ 平方欧几里得距离 $\\left|x\\right|^2$ 马氏距离 $\\mathbf{X}^TA\\mathbf{X}$ KL 散度 $\\sum_{i=1}^{n}{x_i\\log{x_i}}$ Wasserstein 距离（Wasserstein Distance） Wasserstein 距离被称为推土机距离，用于表示两个分布的相似度。Wasserstein 距离定义为把分布$X$转变成分布$Y$所需移动的平均距离的最小值，如下图所示：\nWasserstein 距离（Wasserstein Distance）\n对于$n$维空间中的任意两个分布$X$和$Y$的 Wasserstein 距离定义为：\n$$d=\\inf_{\\gamma\\sim\\Pi(X,Y)}{\\mathrm{E}_{p,q\\sim\\gamma}\\left|p-q\\right|}$$\n其中，$\\Pi(X,Y)$为分布$X$和分布$Y$构成的联合分布的集合，$\\gamma$为$\\Pi(X,Y)$中任意分布，$p$和$q$是分布$\\gamma$中的样本。\n巴氏距离（Bhattacharyya Distance） 巴氏系数可以用来度量两个分布的相似性，对于$n$维空间中的任意两个分布$X$和$Y$的巴氏系数定义为：\n$$c_b=\\int_{p}{\\sqrt{X(p)\\cdot Y(p)}\\mathrm{d}p}$$\n巴氏距离定义为：\n$$d_b=-\\ln{c_b}$$\n需要注意的是，海林格距离$d=\\sqrt{1-d_b}$。在 Python 中用于计算任意两个分布间巴氏距离的代码如下：\nimport numpy as np def BhattacharyyaCoefficient(p,q): p = np.array(p) q = np.array(q) return np.sum(np.sqrt(p * q)) def HellingerDistance(p, q): return np.sqrt(1 - BhattacharyyaCoefficient(p, q)) def BhattacharyyaDistance(p, q): return np.log(BhattacharyyaCoefficient(p, q)) 最大均值差异（Maximum Mean Discrepancy） 最大均值差异是迁移学习领域中最为广泛使用的一种损失函数，它度量了再生希尔伯特空间中两个不同分布间的距离。通过在样本空间寻找连续函数$f:X\\rightarrow R$随机投影后，分别求这两个分布在$f$上函数值的均值，并通过做差得到均值差异（Mean Discrepancy）。最大化均值差异即为寻找一个$f$使得均值差异最大。对于$n$维空间中的任意两个分布$X$和$Y$的最大均值差异定义为：\n$$d=\\sup_{|f|_\\mathrm{H} \\leq 1}{\\mathrm{E}_p[f(X)]-\\mathrm{E}_q[f(Y)]}$$\n点间互信息（Pointwise Mutual Information） 点间互信息用来衡量两个分布的相关性，对于$n$维空间中的任意两个分布$X$和$Y$的点间互信息定义为：\n$$d=\\log{\\frac{p(X,Y)}{p(X)\\cdot p(Y)}}=\\log{\\frac{p(X|Y)}{p(X)}}=\\log{\\frac{p(Y|X)}{p(Y)}}$$\n若$X$和$Y$不相关，则$P(X,Y)=P(X)\\cdot P(Y)$。\n","wordCount":"1020","inLanguage":"en","datePublished":"2023-08-18T11:27:00+08:00","dateModified":"2023-08-18T11:27:00+08:00","author":{"@type":"Person","name":"Kai Wang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kwang.life/2023/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%B1%87%E6%80%BB/"},"publisher":{"@type":"Organization","name":"退思轩","logo":{"@type":"ImageObject","url":"https://kwang.life/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><header class=header><nav class=nav><div class=logo><a href=https://kwang.life accesskey=h title="退思轩 (Alt + H)"><img src=https://kwang.life/apple-touch-icon.png alt aria-label=logo height=35>退思轩</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kwang.life/archives/ title=📔归档><span>📔归档</span></a></li><li><a href=https://kwang.life/tags/ title=🏷️标签><span>🏷️标签</span></a></li><li><a href=https://kwang.life/search/ title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://kwang.life/categories/ title=🗂️分类><span>🗂️分类</span></a></li><li><a href=https://kwang.life/friends/ title=🏡邻居><span>🏡邻居</span></a></li><li><a href=https://kwang.life/about/ title=📄关于><span>📄关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>机器学习中常用的距离度量汇总</h1><div class=post-meta><span title='2023-08-18 11:27:00 +0800 +0800'>2023年8月18日</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1020 words&nbsp;·&nbsp;Kai Wang&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2023/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#距离的定义>距离的定义</a></li><li><a href=#空间距离>空间距离</a><ul><li><a href=#欧几里得距离euclidean-distance>欧几里得距离（Euclidean Distance）</a></li><li><a href=#曼哈顿距离manhattan-distance>曼哈顿距离（Manhattan Distance）</a></li><li><a href=#切比雪夫距离chebyschev-distance>切比雪夫距离（Chebyschev Distance）</a></li><li><a href=#闵可夫斯基距离minkowski-distance>闵可夫斯基距离（Minkowski Distance）</a></li><li><a href=#标准化欧几里得距离standardized-euclidean-distance>标准化欧几里得距离（Standardized Euclidean Distance）</a></li><li><a href=#马氏距离mahalanobis-distance>马氏距离（Mahalanobis Distance）</a></li><li><a href=#兰氏距离lance-and-williams-distance>兰氏距离（Lance and Williams Distance）</a></li><li><a href=#余弦相似度cosine-similarity>余弦相似度（Cosine Similarity）</a></li><li><a href=#测地距离geodesic-distance>测地距离（Geodesic Distance）</a></li><li><a href=#布雷柯蒂斯距离bray-curtis-distance>布雷柯蒂斯距离（Bray Curtis Distance）</a></li><li><a href=#半正矢距离haversine-distance>半正矢距离（Haversine Distance）</a></li></ul></li><li><a href=#字符距离>字符距离</a><ul><li><a href=#汉明距离hamming-distance>汉明距离（Hamming Distance）</a></li><li><a href=#莱文斯坦距离levenshtein-distance>莱文斯坦距离（Levenshtein Distance）</a></li><li><a href=#归一化-google-距离normalized-google-distance>归一化 Google 距离（Normalized Google Distance）</a></li><li><a href=#jaro-winkler-相似度jaro-winkler-similarity>Jaro-Winkler 相似度（<strong>J</strong>aro-Winkler Sim<strong>ilarity</strong>）</a></li><li><a href=#李距离lee-distance>李距离（Lee Distance）</a></li></ul></li><li><a href=#集合距离>集合距离</a><ul><li><a href=#杰卡德相似系数jaccard-similarity-coefficient>杰卡德相似系数（Jaccard Similarity Coefficient）</a></li><li><a href=#奥奇亚系数ochiia-coefficient>奥奇亚系数（Ochiia Coefficient）</a></li><li><a href=#戴斯系数dice-coefficient>戴斯系数（Dice Coefficient）</a></li><li><a href=#豪斯多夫距离hausdorff-distance>豪斯多夫距离（Hausdorff Distance）</a></li></ul></li><li><a href=#分布距离>分布距离</a><ul><li><a href=#皮尔逊相关系数pearson-correlation-coefficient>皮尔逊相关系数（Pearson Correlation Coefficient）</a></li><li><a href=#卡方度量chi-square-measure>卡方度量（Chi-square Measure）</a></li><li><a href=#交叉熵cross-entropy>交叉熵（Cross Entropy）</a></li><li><a href=#kl-散度kullback-leibler-divergence>KL 散度（Kullback-Leibler Divergence）</a></li><li><a href=#js-散度jensen-shannon-divergence>JS 散度（Jensen-Shannon Divergence）</a></li><li><a href=#海林格距离hellinger-distance>海林格距离（Hellinger Distance）</a></li><li><a href=#α-散度α-divergence>α 散度（α Divergence）</a></li><li><a href=#f-散度f-divergence>F 散度（F Divergence）</a></li><li><a href=#布雷格曼散度bregman-divergence>布雷格曼散度（Bregman Divergence）</a></li><li><a href=#wasserstein-距离wasserstein-distance>Wasserstein 距离（Wasserstein Distance）</a></li><li><a href=#巴氏距离bhattacharyya-distance>巴氏距离（Bhattacharyya Distance）</a></li><li><a href=#最大均值差异maximum-mean-discrepancy>最大均值差异（Maximum Mean Discrepancy）</a></li><li><a href=#点间互信息pointwise-mutual-information>点间互信息（Pointwise Mutual Information）</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=距离的定义>距离的定义<a hidden class=anchor aria-hidden=true href=#距离的定义>#</a></h2><p>在机器学习中，我们通过计算不同样本在特征空间中的距离来评估样本间的相似度，进而为其进行分类。根据样本特征空间的不同，我们需要选择合适的距离度量方法。一般而言，对于<strong>距离度量函数</strong>$d(x,y)$，其需要满足如下性质：</p><ul><li><strong>非负性</strong>：$d(x,y)\geq 0$</li><li><strong>同一性</strong>：$d(x,y)=0\Leftrightarrow x=y$</li><li><strong>对称性</strong>：$d(x,y)=d(y,x)$</li><li><strong>三角不等式</strong>：$d(x,y)\leq d(x,z)+d(z,y)$</li></ul><p>根据样本特征空间的不同，我们把度量的距离分为：<strong>空间距离</strong>、<strong>字符距离</strong>、<strong>集合距离</strong>、<strong>分布距离</strong>。</p><h2 id=空间距离>空间距离<a hidden class=anchor aria-hidden=true href=#空间距离>#</a></h2><h3 id=欧几里得距离euclidean-distance>欧几里得距离（Euclidean Distance）<a hidden class=anchor aria-hidden=true href=#欧几里得距离euclidean-distance>#</a></h3><p>欧几里得距离用于描述欧式空间中任意两点间的直线距离，常被称作欧几里得范数，或$\mathcal{L}_2$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间欧几里得距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>EuclideanDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>)))</span>
</span></span></code></pre></div><h3 id=曼哈顿距离manhattan-distance>曼哈顿距离（Manhattan Distance）<a hidden class=anchor aria-hidden=true href=#曼哈顿距离manhattan-distance>#</a></h3><p>曼哈顿距离用于描述标准坐标系中任意两点间的绝对轴距之和，常被称作曼哈顿范数，或$\mathcal{L}_1$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的曼哈顿距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left|x_i-y_i\right|}}$$</p><p>下图所示红色路径为曼哈顿距离，绿色路径为欧几里得距离，蓝色路径和黄色路径为等价曼哈顿距离。曼哈顿距离的发明是为了应对早期计算图形学领域中浮点运算代价高的情况，采用曼哈顿距离代替欧几里得距离进行图形渲染。</p><figure><img loading=lazy src=/images/2023/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb/Manhattan-Distance.png alt="曼哈顿距离（Manhattan Distance）"><figcaption><p>曼哈顿距离（Manhattan Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间曼哈顿距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ManhattanDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=切比雪夫距离chebyschev-distance>切比雪夫距离（Chebyschev Distance）<a hidden class=anchor aria-hidden=true href=#切比雪夫距离chebyschev-distance>#</a></h3><p>切比雪夫距离用于描述标准坐标系中任意两点间的绝对轴距最大值，常被称作切比雪夫范数，或$\mathcal{L}_\infty$<strong>范数</strong>。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的切比雪夫距离定义为：</p><p>$$d=\max\left|x_i-y_i\right|$$</p><p>以下图所示位于 F6 的“王”为例，其到棋盘上任意一点的切比雪夫距离即为它需要移动的步数。需要注意的是，在国际象棋中“王”每次可以朝任意方向移动一格。</p><figure><img loading=lazy src=/images/2023/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb/Chebyschev-Distance.png alt="切比雪夫距离（Chebyschev Distance）"><figcaption><p>切比雪夫距离（Chebyschev Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间切比雪夫距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ChebyshevDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=闵可夫斯基距离minkowski-distance>闵可夫斯基距离（Minkowski Distance）<a hidden class=anchor aria-hidden=true href=#闵可夫斯基距离minkowski-distance>#</a></h3><p>闵可夫斯基距离并非一种新型的距离度量方式，而是一种对于多种不同距离度量的概括性表述。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的闵可夫斯基距离定义为：</p><p>$$d=\left(\sum_{i=1}^{n}{\left|x_i-y_i\right|^p}\right)^{1/p}$$</p><p>其中，$p$为闵可夫斯基距离参数。当$p=1$时，闵可夫斯基距离退化为曼哈顿距离；当$p=2$时，闵可夫斯基距离退化为欧几里得距离；当$p\rightarrow\infty$时，闵可夫斯基距离退化为切比雪夫距离，如下图所示：</p><figure><img loading=lazy src=/images/2023/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb/Minkowski-Distance.png alt="闵可夫斯基距离（Minkowski Distance）"><figcaption><p>闵可夫斯基距离（Minkowski Distance）</p></figcaption></figure><p>在 <code>Python</code> 中用于计算任意两点间闵可夫斯基距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>MinkowskiDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>zipped_coordinate</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>math</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>),</span> <span class=n>p</span><span class=p>)),</span> <span class=mi>1</span> <span class=o>/</span> <span class=n>p</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=标准化欧几里得距离standardized-euclidean-distance>标准化欧几里得距离（Standardized Euclidean Distance）<a hidden class=anchor aria-hidden=true href=#标准化欧几里得距离standardized-euclidean-distance>#</a></h3><p>标准化欧几里得距离是将欧式空间中任意两点的分量都“标准化”到均值、方差一致的区间，记每个分量的均值为$\mu$，方差为$\sigma_i^2$，“标准化”结果为$X^*$：</p><p>$$X^*=\frac{X-\mu}{\sigma^2}$$</p><p>对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的标准化欧几里得距离定义为：</p><p>$$d=\sqrt{\sum_{i=1}^{n}{\left(\frac{x_i-y_i}{\sigma_i^2}\right)^2}}$$</p><p>如果将$1/\sigma^2$看做权重，则标准化欧几里得距离可以被认为是加权欧几里得距离。</p><p>在 <code>Python</code> 中用于计算任意两点间标准化欧几里得距离的代码如下（需要注意 <code>sigma ≠ 0</code>）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>StandardizedEuclideanDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>sigma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>ddof</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(((</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span> <span class=o>/</span><span class=n>sigma</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span>
</span></span></code></pre></div><h3 id=马氏距离mahalanobis-distance>马氏距离（Mahalanobis Distance）<a hidden class=anchor aria-hidden=true href=#马氏距离mahalanobis-distance>#</a></h3><p>马氏距离由印度统计学家<a href=https://en.wikipedia.org/wiki/Prasanta_Chandra_Mahalanobis>马哈拉诺比斯（P. C. Mahalanobis）</a>提出，用于表示数据的协方差距离，它可以有效地表示两个未知样本集之间的相似度。对于$n$维空间中的任意两个样本集$\textbf{X}$和$\textbf{Y}$的马氏距离定义为：</p><p>$$d=\sqrt{\left(\textbf{X}-\textbf{Y}\right)^T\Sigma^{-1}\left(\textbf{X}-\textbf{Y}\right)}$$</p><p>其中，$\Sigma$为样本集$X$和$Y$的协方差矩阵。若协方差矩阵为单位矩阵，则马氏距离退化为欧几里得距离；若协方差矩阵为对角矩阵，则马氏距离退化为标准化欧几里得距离。</p><p>需要注意的是，马氏距离的计算需要确保$\Sigma$的逆矩阵存在，否则可以直接采用欧几里得距离进行计算。此外，马氏距离不受量纲的影响，它可以排除变量之间相关性的干扰，但同时也夸大了微小变化量的作用。例如，若将两个相同的样本放入两个不同的总体中，经计算的到的马氏距离也是不同的（除非它们的$\Sigma$恰巧相同）。</p><p>在 <code>Python</code> 中用于计算任意两个样本集间马氏距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>MahalanobisDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>X_T</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>    <span class=n>sigma_inverse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>cov</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>d</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X_T</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X_T</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>            <span class=n>delta</span> <span class=o>=</span> <span class=n>X_T</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>X_T</span><span class=p>[</span><span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>d</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>delta</span><span class=p>,</span><span class=n>sigma_inverse</span><span class=p>),</span><span class=n>delta</span><span class=o>.</span><span class=n>T</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>d</span>
</span></span></code></pre></div><h3 id=兰氏距离lance-and-williams-distance>兰氏距离（Lance and Williams Distance）<a hidden class=anchor aria-hidden=true href=#兰氏距离lance-and-williams-distance>#</a></h3><p>兰氏距离又被称为堪培拉距离（Canberra Distance），可以理解为加权曼哈顿距离。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的兰氏距离定义为：</p><p>$$d=\sum_{i=1}^{n}{\frac{\left|x_i-y_i\right|}{\left|x_i\right|+\left|y_i\right|}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间兰氏距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>CanberraDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>d</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>and</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>d</span> <span class=o>+=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>d</span> <span class=o>+=</span> <span class=nb>abs</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>/</span> <span class=p>(</span><span class=nb>abs</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>+</span> <span class=nb>abs</span><span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>d</span>
</span></span></code></pre></div><h3 id=余弦相似度cosine-similarity>余弦相似度（Cosine Similarity）<a hidden class=anchor aria-hidden=true href=#余弦相似度cosine-similarity>#</a></h3><p>在几何学中，通常采用余弦相似度来度量两个向量间的夹角，其取值为$[-1,1]$。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的余弦相似度定义为：</p><p>$$\cos{\left(\vec{x},\vec{y}\right)}=\frac{\vec{x}\cdot\vec{y}}{\left|\vec{x}\right|\cdot\left|\vec{y}\right|}=\frac{\displaystyle\sum_{i=1}^{n}{x_i\cdot y_i}}{\sqrt{\displaystyle\sum_{i=1}^{n}{x_i^2}}\cdot\sqrt{\displaystyle\sum_{i=1}^{n}{y_i^2}}}$$</p><p>当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow0$时，两向量完全正交；当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow1$时，两向量完全重合；当$\cos{\left(\vec{x},\vec{y}\right)}\rightarrow -1$时，两向量完全相反。在 <code>Python</code> 中用于计算任意两点间余弦相似度的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>CosineDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>y</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=测地距离geodesic-distance>测地距离（Geodesic Distance）<a hidden class=anchor aria-hidden=true href=#测地距离geodesic-distance>#</a></h3><p>测地距离原指球体表面上两点间的最短距离，后来被推广到其它领域。在图论中，测地距离为两顶点间的最短路径；在欧式空间中，测地距离为欧几里得距离；在非欧空间中，测地距离为连接两点间的最短圆弧。如下图所示：</p><figure><img loading=lazy src=/images/2023/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb/Geodesic-Distance.jpg alt="测地距离（Geodesic Distance）"><figcaption><p>测地距离（Geodesic Distance）</p></figcaption></figure><h3 id=布雷柯蒂斯距离bray-curtis-distance>布雷柯蒂斯距离（Bray Curtis Distance）<a hidden class=anchor aria-hidden=true href=#布雷柯蒂斯距离bray-curtis-distance>#</a></h3><p>布雷柯蒂斯距离主要用于生态学和环境科学领域，用于计算不同样本间的差异。对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的布雷柯蒂斯距离定义为：</p><p>$$d=\frac{\displaystyle \sum_{i=1}^{n}{\left|x_i-y_i\right|}}{\displaystyle \sum_{i=1}^{n}{x_i}+\sum_{i=1}^{n}{y_i}}$$</p><p>在 <code>Python</code> 中用于计算任意两点间布雷柯蒂斯距离的代码如下：</p><pre tabindex=0><code class=language-text-plain data-lang=text-plain>import numpy as np

def BrayCurtisDistance(x, y):
    x = np.array(x)
    y = np.array(y)
    return np.sum(np.abs(x - y)) / (np.sum(x) + np.sum(y))
</code></pre><h3 id=半正矢距离haversine-distance>半正矢距离（Haversine Distance）<a hidden class=anchor aria-hidden=true href=#半正矢距离haversine-distance>#</a></h3><p>半正矢距离用于计算任意两经纬点间的距离，对于空间中的任意两经纬点$A(\mathrm{lon}_1,\mathrm{lat}_1)$和$B(\mathrm{lon}_2,\mathrm{lat}_2)$的半正矢距离定义为：</p><p>$$d=2r\cdot\arcsin{\sqrt{\sin^2{\frac{\mathrm{lat}_2-\mathrm{lat}_1}{2}}+\cos{(\mathrm{lat}_1)}\cos{(\mathrm{lat}_2)}\sin^2{\frac{\mathrm{lon}_2-\mathrm{lon}_1}{2}}}}$$</p><p>其中，$r$为半径。在 <code>Python</code> 中用于计算任意两经纬点间半正矢距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>HaversineDistance</span><span class=p>(</span><span class=n>lon1</span><span class=p>,</span> <span class=n>lat1</span><span class=p>,</span> <span class=n>lon2</span><span class=p>,</span> <span class=n>lat2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>lon1</span><span class=p>,</span> <span class=n>lat1</span><span class=p>,</span> <span class=n>lon2</span><span class=p>,</span> <span class=n>lat2</span> <span class=o>=</span> <span class=nb>map</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>radians</span><span class=p>,</span> <span class=p>[</span><span class=n>lon1</span><span class=p>,</span> <span class=n>lat1</span><span class=p>,</span> <span class=n>lon2</span><span class=p>,</span> <span class=n>lat2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>c</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=mi>6367</span> <span class=o>*</span> <span class=mi>1000</span> <span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>arcsin</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>((</span><span class=n>lat2</span> <span class=o>-</span> <span class=n>lat1</span><span class=p>)</span> <span class=o>/</span> <span class=mf>2.0</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>lat1</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>lat2</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>((</span><span class=n>lon2</span> <span class=o>-</span> <span class=n>lon1</span><span class=p>)</span> <span class=o>/</span> <span class=mf>2.0</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>))</span>
</span></span></code></pre></div><h2 id=字符距离>字符距离<a hidden class=anchor aria-hidden=true href=#字符距离>#</a></h2><h3 id=汉明距离hamming-distance>汉明距离（Hamming Distance）<a hidden class=anchor aria-hidden=true href=#汉明距离hamming-distance>#</a></h3><p>汉明距离由两个字符串对应位不同的数量决定，通常用于数据传输差错控制编码领域。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的汉明距离定义为：</p><p>$$d=\sum_{i=0}^{n}{x_i\otimes y_i}$$</p><p>汉明距离也可以理解为将$A$变为$B$的最小操作次数。在 <code>Python</code> 中用于计算任意两个字符串间汉明距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>HammingDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>x_ch</span> <span class=o>!=</span> <span class=n>y_ch</span> <span class=k>for</span> <span class=n>x_ch</span><span class=p>,</span> <span class=n>y_ch</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=莱文斯坦距离levenshtein-distance>莱文斯坦距离（Levenshtein Distance）<a hidden class=anchor aria-hidden=true href=#莱文斯坦距离levenshtein-distance>#</a></h3><p>莱文斯坦距离又被称为编辑距离（Edit Distance），用于度量两个字符串之间的差异，定义为：将字符串$A$转化为字符串$B$所需的最少单字符编辑（插入、删除或替换）次数。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$，$A$的前$i$个字符和$B$的前$j$个字符的莱文斯坦距离定义为：</p><p>$$d(i,j) = \begin{align*} \left{\begin{matrix} \max{\left(i,j\right)}, & \min{\left(i,j\right)}=0 \ \min{\left[d(i-1,j),d(i,j-1), d(i,j)\right]} + I(i,j), & \min{\left(i,j\right)\neq 0} \end{matrix}\right. \end{align*}$$</p><p>其中，$I(\cdot)$为指示函数，当$x_i=y_j$时，$I(i,j)=0$；当$x_i\neq y_j$时，$I(i,j)=1$。在 <code>Python</code> 中用于计算任意两个字符串间莱文斯坦距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>LevenshteinDistance</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>dp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>i</span>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>dp</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>j</span>
</span></span><span class=line><span class=cl>     
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>delta</span> <span class=o>=</span> <span class=mi>0</span> <span class=k>if</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=n>y</span><span class=p>[</span><span class=n>j</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=k>else</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>dp</span><span class=p>[</span><span class=n>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>][</span><span class=n>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>delta</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>dp</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>int</span><span class=p>(</span><span class=n>dp</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>)][</span><span class=nb>len</span><span class=p>(</span><span class=n>y</span><span class=p>)])</span>
</span></span></code></pre></div><h3 id=归一化-google-距离normalized-google-distance>归一化 Google 距离（Normalized Google Distance）<a hidden class=anchor aria-hidden=true href=#归一化-google-距离normalized-google-distance>#</a></h3><p>归一化 Google 距离是由给定一组关键词集合的 Google 搜索引擎所返回的命中数量决定的，它是一种语义相似度量方法。对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的归一化 Google 距离定义为：</p><p>$$d=\frac{\max{\left[\log{f(A)},\log{f(B)}\right]}-\log{f(A,B)}}{\log{M}-\min{\left[\log{f(A)}, \log{f(B)}\right]}}$$</p><p>其中，$M$为 Google 搜索引擎所返回的网页总数；$f(x)$和$f(y)$分别为 Google 搜索引擎返回关于字符串$A$和字符串$B$的命中数量；$f(A,B)$为 Google 搜索引擎返回关于字符串$A$和字符串$B$同时出现的命中数量。</p><h3 id=jaro-winkler-相似度jaro-winkler-similarity>Jaro-Winkler 相似度（<strong>J</strong>aro-Winkler Sim<strong>ilarity</strong>）<a hidden class=anchor aria-hidden=true href=#jaro-winkler-相似度jaro-winkler-similarity>#</a></h3><p>Jaro 相似度用于评估两个字符串间的相似度，对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的 Jaro 相似度定义为：</p><p>$$d_j=\frac{1}{3}\cdot\left(\frac{m}{|A|}+\frac{m}{|B|}+\frac{m-t}{m}\right)$$</p><p>其中，$m$是匹配的字符数，$t$是替换的字符数。</p><p>Jaro-Winkler 相似度在 Jaro 相似度的基础上引入了前缀的概念，对于长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的 Jaro-Winkler 相似度定义为：</p><p>$$d_w=d_j+l\cdot p\cdot(1-d_j)$$</p><p>其中，$l$为字符串$A$和字符串$B$的共同前缀的字符数；$p$为缩放因子（常取$p=0.1$）。</p><h3 id=李距离lee-distance>李距离（Lee Distance）<a hidden class=anchor aria-hidden=true href=#李距离lee-distance>#</a></h3><p>李距离是编码理论中用于描述字符串距离的方案，对于使用包含$q$个字母的字母表且长度为$n$的任意两个字符串$A=x_1x_2\cdots x_n$和$B=y_1y_2\cdots y_n$的李距离定义为：</p><p>$$d=\sum_{i=0}^{n}{\min{\left(\left|x_i-y_i\right|,q-\left|x_i-y_i\right|\right)}}$$</p><p>当$q=2$或$q=3$时，李距离退化为汉明距离。</p><h2 id=集合距离>集合距离<a hidden class=anchor aria-hidden=true href=#集合距离>#</a></h2><h3 id=杰卡德相似系数jaccard-similarity-coefficient>杰卡德相似系数（Jaccard Similarity Coefficient）<a hidden class=anchor aria-hidden=true href=#杰卡德相似系数jaccard-similarity-coefficient>#</a></h3><p>杰卡德相似系数是指两个集合$A$和$B$中相同元素在所有元素中的占比，定义为$J(A,B)$：</p><p>$$J(A,B)=\frac{\left|A\cap B\right|}{\left|A\cup B\right|}$$</p><p>杰卡德距离（Jaccard Distance）是指两个集合$A$和$B$中不同元素在所有元素中的占比，定义为$J_\delta(A,B)$：</p><p>$$J_\delta(A,B)=1-J(A,B)=1-\frac{\left|A\cap B\right|}{\left|A\cup B\right|}$$</p><p>在 <code>Python</code> 中用于计算任意两个集合间杰卡德相似系数的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>JaccardSimilarityCoefficient</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>double</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>bitwise_and</span><span class=p>((</span><span class=n>x</span> <span class=o>!=</span> <span class=n>y</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>bitwise_or</span><span class=p>(</span><span class=n>x</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>y</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>))</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>double</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>bitwise_or</span><span class=p>(</span><span class=n>x</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>y</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>())</span>
</span></span></code></pre></div><h3 id=奥奇亚系数ochiia-coefficient>奥奇亚系数（Ochiia Coefficient）<a hidden class=anchor aria-hidden=true href=#奥奇亚系数ochiia-coefficient>#</a></h3><p>奥奇亚系数是指两个集合$A$和$B$中相同元素与两集合大小几何平均值的比值，定义为$K(A,B)$：</p><p>$$K(A,B)=\frac{\left|A\cap B\right|}{\sqrt{\left|A\right|\times\left|B\right|}}$$</p><h3 id=戴斯系数dice-coefficient>戴斯系数（Dice Coefficient）<a hidden class=anchor aria-hidden=true href=#戴斯系数dice-coefficient>#</a></h3><p>戴斯系数除了可以用来衡量两个集合间的距离，还可以用来衡量两个字符串间的距离，定义为$D(A,B)$：</p><p>$$D(A,B)=\frac{2\left|A\cap B\right|}{\left|A\right|+\left|B\right|}$$</p><h3 id=豪斯多夫距离hausdorff-distance>豪斯多夫距离（Hausdorff Distance）<a hidden class=anchor aria-hidden=true href=#豪斯多夫距离hausdorff-distance>#</a></h3><p>豪斯多夫距离用于度量两个集合$A$和$B$间的距离，定义为$H(A,B)$：</p><p>$$H(A,B)=\max{\left[h(A,B),h(B,A)\right]}$$</p><p>其中，$h(\cdot)$为双向豪斯多夫距离，定义为：</p><p>$$h(A,B)=\max_{a\in A}{\min_{b\in B}{\left |a-b\right|}}$$</p><p>其中，$h(A,B)$为从集合$A$到集合$B$的单向豪斯多夫距离，$h(B,A)$为从集合$B$到集合$A$的单向豪斯多夫距离。</p><h2 id=分布距离>分布距离<a hidden class=anchor aria-hidden=true href=#分布距离>#</a></h2><h3 id=皮尔逊相关系数pearson-correlation-coefficient>皮尔逊相关系数（Pearson Correlation Coefficient）<a hidden class=anchor aria-hidden=true href=#皮尔逊相关系数pearson-correlation-coefficient>#</a></h3><p>皮尔逊相关系数又被称为皮尔逊积矩相关技术（Pearson Product Moment Correlation Coefficient，PPMCC/PCC），用于度量两个变量$X$和$Y$之间的线性相关性。与上文提到的余弦相似度不同，皮尔逊相关系数不受平移变换的影响。对于$n$维空间中的任意两个分布$X$和$Y$的皮尔逊相关系数定义为：</p><p>$$\rho(X,Y)=\frac{\mathrm{cov}(X,Y)}{\sigma(X)\cdot\sigma(Y)}=\frac{\mathrm{E}\left[\left(X-X_\mu\right)\left(Y-Y_\mu\right)\right]}{\sigma(X)\cdot\sigma(Y)}$$</p><p>皮尔逊相关系数与余弦相似度的关系为：</p><p>$$\rho(X,Y)=\frac{\mathrm{E}\left[\left(X-X_\mu\right)\left(Y-Y_\mu\right)\right]}{\sigma(X)\cdot\sigma(Y)}=\frac{\displaystyle\sum_{i=1}^{n}{\left(X-X_\mu\right)\cdot\left(Y-Y_\mu\right)}}{\left|X-X_\mu\right|\cdot\left|Y-Y_\mu\right|}=\cos{\left(X-X_\mu,Y-Y_\mu\right)}$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间皮尔逊相关系数的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>PearsonCorrelationCoefficient</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y_</span> <span class=o>=</span> <span class=n>y</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_</span><span class=p>,</span> <span class=n>y_</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x_</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>y_</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=卡方度量chi-square-measure>卡方度量（Chi-square Measure）<a hidden class=anchor aria-hidden=true href=#卡方度量chi-square-measure>#</a></h3><p>$\mathcal{X}^2$检验通常用于检验某一观测分布是否符合典型理论分布。若观测频数与期望频数差异越小，则$\mathcal{X}^2$值越小；若观测频数与期望频数差异越大，则$\mathcal{X}^2$值越大。因此，$\mathcal{X}^2$值可以用于描述观测分布与理论分布的差异。对于$n$维空间中的任意两个分布$X$和$Y$的$\mathcal{X}^2$统计量定义为：</p><p>$$\mathcal{X}^2=\sum_{i=1}^{n}{\frac{\left(x_i-y_i\right)^2}{y_i}}=\sum_{i=1}^{k}{\frac{\left(x_i-n\cdot p_i\right)^2}{k\cdot p_i}}$$</p><p>其中，$x_i$为$X$在$i$的频数，$y_i$为$Y$在$i$的频数，$k$为总频数，$p_i$为$Y$在$i$的概率。在 <code>Python</code> 中用于计算任意两个分布间卡方度量的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ChiSquareMeasure</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>)</span> <span class=o>/</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=交叉熵cross-entropy>交叉熵（Cross Entropy）<a hidden class=anchor aria-hidden=true href=#交叉熵cross-entropy>#</a></h3><p>交叉熵是香农信息论中的重要概念，用于度量两个分布之间的差异信息。对于$n$维空间中的任意两个分布$X$和$Y$的交叉熵定义为：</p><p>$$H(X,Y)=-\int_{p}{X(p)\cdot Y(p)\mathrm{d}p}$$</p><p>若基于分布$X$对分布$X$进行编码，其编码长度的期望为：</p><p>$$H(X)=-\int_{p}{X(p)\mathrm{d}p}$$</p><p>若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$。在 <code>Python</code> 中用于计算任意两个分布间交叉熵的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>CrossEntropy</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>x</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>y</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=kl-散度kullback-leibler-divergence>KL 散度（Kullback-Leibler Divergence）<a hidden class=anchor aria-hidden=true href=#kl-散度kullback-leibler-divergence>#</a></h3><p>KL 散度又被称为相对熵（Relative Entropy）或信息散度（Information Divergence），用于度量两个分布间的差异，对于$n$维空间中的任意两个分布$X$和$Y$的 KL 散度定义为：</p><p>$$\mathrm{KL}(X|Y)=\int_{p}{X(p)\cdot\log{\frac{X(p)}{Y(p)}}\mathrm{d}p}$$</p><p>若基于分布$X$对分布$X$进行编码，其编码长度的期望为：</p><p>$$H(X)=-\int_{p}{X(p)\log{X(p)}\mathrm{d}p}$$</p><p>若基于分布$Y$对分布$X$进行编码，其编码长度的期望即为交叉熵$H(X,Y)$，其多出的编码长度为：</p><p>$$\mathrm{KL}(X|Y)=H(X)-H(X,Y)$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间 KL 散度的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>KullbackLeiblerDivergence</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>p</span> <span class=o>/</span> <span class=n>q</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=js-散度jensen-shannon-divergence>JS 散度（Jensen-Shannon Divergence）<a hidden class=anchor aria-hidden=true href=#js-散度jensen-shannon-divergence>#</a></h3><p>JS 散度是 KL 散度的变体，解决了 KL 散度非对称的问题，对于$n$维空间中的任意两个分布$X$和$Y$的 JS 散度定义为：</p><p>$$\mathrm{JS}(X|Y)=\frac{1}{2}\cdot\mathrm{KL}(X|\frac{X+Y}{2})+\frac{1}{2}\cdot\mathrm{KL}(Y|\frac{X+Y}{2})$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间 JS 散度的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>JensenShannonDivergence</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>p</span><span class=o>/</span><span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>q</span><span class=p>)))</span> <span class=o>+</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>q</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>q</span><span class=o>/</span><span class=p>(</span><span class=n>p</span> <span class=o>+</span> <span class=n>q</span><span class=p>)))</span>
</span></span></code></pre></div><h3 id=海林格距离hellinger-distance>海林格距离（Hellinger Distance）<a hidden class=anchor aria-hidden=true href=#海林格距离hellinger-distance>#</a></h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的海林格距离定义为：</p><p>$$d={\frac{1}{\sqrt{2}}\cdot\sqrt{\sum_{i=1}^{n}{\left(\sqrt{x_i}-\sqrt{y_i}\right)^2}}}$$</p><p>在 <code>Python</code> 中用于计算任意两个分布间海林格距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>HellingerDistance</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>p</span><span class=p>)</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>q</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=α-散度α-divergence>α 散度（α Divergence）<a hidden class=anchor aria-hidden=true href=#α-散度α-divergence>#</a></h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的 α 散度被定义为：</p><p>$$d=\frac{4}{1-\alpha^2}\cdot\left[1-\int_p{X(p)^{(1+\alpha)/2}\cdot Y(p)^{(1-\alpha)/2}}\mathrm{d}x\right]$$</p><p>其中，$-\infty&lt;\alpha&lt;+\infty$为连续参数。当$\alpha\rightarrow 1$时，α 散度退化为 KL 散度；当$\alpha\rightarrow 0$时，α 散度退化为海林格距离（仅相差常系数）。</p><h3 id=f-散度f-divergence>F 散度（F Divergence）<a hidden class=anchor aria-hidden=true href=#f-散度f-divergence>#</a></h3><p>对于$n$维空间中的任意两个分布$X$和$Y$的 F 散度被定义为：</p><p>$$\mathrm{D}(X|Y)=\int_p{Y(p)\cdot f\left[\frac{X(p)}{Y(p)}\right]\mathrm{d}p}$$</p><p>其中，函数$f(\cdot)$需满足：（1）$f(\cdot)$为凸函数；（2）$f(1)=0$。下表给出了$f(\cdot)$取不同值时，F 散度对应的结果。</p><table><thead><tr><th><strong>散度</strong></th><th>$f(\cdot)$</th></tr></thead><tbody><tr><td>卡方距离</td><td>$(t-1)^2$</td></tr><tr><td>KL 散度</td><td>$x\log{x}$</td></tr><tr><td>逆 KL 散度</td><td>$-\log{x}$</td></tr><tr><td>海林格距离</td><td>$\left(\sqrt{x}-1\right)^2$</td></tr><tr><td>α 散度</td><td>$4/(1-\alpha)^2\cdot\left(1-x^{(1+\alpha)/2}\right)$</td></tr></tbody></table><p>在 <code>Python</code> 中用于计算任意两个分布间 F 散度（海林格距离）的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>t</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>FDivergence</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>q</span> <span class=o>*</span> <span class=n>f</span><span class=p>(</span><span class=n>p</span> <span class=o>/</span> <span class=n>q</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=布雷格曼散度bregman-divergence>布雷格曼散度（Bregman Divergence）<a hidden class=anchor aria-hidden=true href=#布雷格曼散度bregman-divergence>#</a></h3><p>对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的欧几里得距离定义为：</p><p>$$d(x,y)=\sqrt{\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}}$$</p><p>将上式两侧平方后，得到：</p><p>$$d^2(x,y)=\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}$$</p><p>定义$&lt;x,y>=\sum_{i=1}^{n}{x_i\cdot y_i}$，$\left|x\right|=\sqrt{&lt;x,x>}$，上式可改写为：</p><p>$$d^2(x,y)=\sum_{i=1}^{n}{\left(x_i-y_i\right)^2}=&lt;x-y,x-y>=\left|x\right|^2-\left(\left|y\right|^2+&lt;2y,x-y>\right)$$</p><p>此处的距离即为欧几里得模函数和其在$x$处切线在$y$处的点估计差。推广该概念后，对于$n$维空间中的任意两点$A(x_1,x_2,\cdots,x_n)$和$B(y_1,y_2,\cdots,y_n)$的布雷格曼散度定义为：</p><p>$$d(x,y)=f(x)-\left[f(y)+&lt;\nabla f(x),x-y >\right]$$</p><p>表给出了$f(\cdot)$取不同值时，布雷格曼散度对应的结果。</p><table><thead><tr><th><strong>散度</strong></th><th>$f(\cdot)$</th></tr></thead><tbody><tr><td>平方损失</td><td>$x^2$</td></tr><tr><td>/</td><td>$x\log{x}$</td></tr><tr><td>Logistic 损失</td><td>$x\log{x}+(1-x)\log{(1-x)}$</td></tr><tr><td>Itakura-Saito 距离</td><td>$-\log{x}$</td></tr><tr><td>/</td><td>$e^x$</td></tr><tr><td>平方欧几里得距离</td><td>$\left|x\right|^2$</td></tr><tr><td>马氏距离</td><td>$\mathbf{X}^TA\mathbf{X}$</td></tr><tr><td>KL 散度</td><td>$\sum_{i=1}^{n}{x_i\log{x_i}}$</td></tr></tbody></table><h3 id=wasserstein-距离wasserstein-distance>Wasserstein 距离（Wasserstein Distance）<a hidden class=anchor aria-hidden=true href=#wasserstein-距离wasserstein-distance>#</a></h3><p>Wasserstein 距离被称为推土机距离，用于表示两个分布的相似度。Wasserstein 距离定义为把分布$X$转变成分布$Y$所需移动的平均距离的最小值，如下图所示：</p><figure><img loading=lazy src=/images/2023/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb/Wasserstein-Distance.png alt="Wasserstein 距离（Wasserstein Distance）"><figcaption><p>Wasserstein 距离（Wasserstein Distance）</p></figcaption></figure><p>对于$n$维空间中的任意两个分布$X$和$Y$的 Wasserstein 距离定义为：</p><p>$$d=\inf_{\gamma\sim\Pi(X,Y)}{\mathrm{E}_{p,q\sim\gamma}\left|p-q\right|}$$</p><p>其中，$\Pi(X,Y)$为分布$X$和分布$Y$构成的联合分布的集合，$\gamma$为$\Pi(X,Y)$中任意分布，$p$和$q$是分布$\gamma$中的样本。</p><h3 id=巴氏距离bhattacharyya-distance>巴氏距离（Bhattacharyya Distance）<a hidden class=anchor aria-hidden=true href=#巴氏距离bhattacharyya-distance>#</a></h3><p>巴氏系数可以用来度量两个分布的相似性，对于$n$维空间中的任意两个分布$X$和$Y$的巴氏系数定义为：</p><p>$$c_b=\int_{p}{\sqrt{X(p)\cdot Y(p)}\mathrm{d}p}$$</p><p>巴氏距离定义为：</p><p>$$d_b=-\ln{c_b}$$</p><p>需要注意的是，海林格距离$d=\sqrt{1-d_b}$。在 <code>Python</code> 中用于计算任意两个分布间巴氏距离的代码如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>BhattacharyyaCoefficient</span><span class=p>(</span><span class=n>p</span><span class=p>,</span><span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>p</span> <span class=o>*</span> <span class=n>q</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>HellingerDistance</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>BhattacharyyaCoefficient</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>BhattacharyyaDistance</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>BhattacharyyaCoefficient</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>q</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=最大均值差异maximum-mean-discrepancy>最大均值差异（Maximum Mean Discrepancy）<a hidden class=anchor aria-hidden=true href=#最大均值差异maximum-mean-discrepancy>#</a></h3><p>最大均值差异是迁移学习领域中最为广泛使用的一种损失函数，它度量了再生希尔伯特空间中两个不同分布间的距离。通过在样本空间寻找连续函数$f:X\rightarrow R$随机投影后，分别求这两个分布在$f$上函数值的均值，并通过做差得到均值差异（Mean Discrepancy）。最大化均值差异即为寻找一个$f$使得均值差异最大。对于$n$维空间中的任意两个分布$X$和$Y$的最大均值差异定义为：</p><p>$$d=\sup_{|f|_\mathrm{H} \leq 1}{\mathrm{E}_p[f(X)]-\mathrm{E}_q[f(Y)]}$$</p><h3 id=点间互信息pointwise-mutual-information>点间互信息（Pointwise Mutual Information）<a hidden class=anchor aria-hidden=true href=#点间互信息pointwise-mutual-information>#</a></h3><p>点间互信息用来衡量两个分布的相关性，对于$n$维空间中的任意两个分布$X$和$Y$的点间互信息定义为：</p><p>$$d=\log{\frac{p(X,Y)}{p(X)\cdot p(Y)}}=\log{\frac{p(X|Y)}{p(X)}}=\log{\frac{p(Y|X)}{p(Y)}}$$</p><p>若$X$和$Y$不相关，则$P(X,Y)=P(X)\cdot P(Y)$。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://kwang.life/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://kwang.life/tags/%E6%B5%8B%E5%BA%A6%E8%AE%BA/>测度论</a></li><li><a href=https://kwang.life/tags/%E7%A9%BA%E9%97%B4%E8%B7%9D%E7%A6%BB/>空间距离</a></li><li><a href=https://kwang.life/tags/%E5%AD%97%E7%AC%A6%E8%B7%9D%E7%A6%BB/>字符距离</a></li><li><a href=https://kwang.life/tags/%E9%9B%86%E5%90%88%E8%B7%9D%E7%A6%BB/>集合距离</a></li><li><a href=https://kwang.life/tags/%E5%88%86%E5%B8%83%E8%B7%9D%E7%A6%BB/>分布距离</a></li></ul><nav class=paginav><a class=next href=https://kwang.life/2022/12/%E9%9B%86%E6%88%90%E7%94%B5%E8%B7%AF%E7%89%88%E5%9B%BE%E8%AE%BE%E8%AE%A1%E5%AE%9E%E9%AA%8C%E4%BB%A5%E8%BF%90%E7%AE%97%E6%94%BE%E5%A4%A7%E5%99%A8%E5%92%8C%E5%B8%A6%E9%9A%99%E5%9F%BA%E5%87%86%E6%BA%90%E4%B8%BA%E4%BE%8B/><span class=title>Next »</span><br><span>集成电路版图设计实验（以运算放大器和带隙基准源为例）</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 机器学习中常用的距离度量汇总 on twitter" href="https://twitter.com/intent/tweet/?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb&url=https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f&hashtags=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%2c%e6%b5%8b%e5%ba%a6%e8%ae%ba%2c%e7%a9%ba%e9%97%b4%e8%b7%9d%e7%a6%bb%2c%e5%ad%97%e7%ac%a6%e8%b7%9d%e7%a6%bb%2c%e9%9b%86%e5%90%88%e8%b7%9d%e7%a6%bb%2c%e5%88%86%e5%b8%83%e8%b7%9d%e7%a6%bb"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 机器学习中常用的距离度量汇总 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb&summary=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb&source=https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 机器学习中常用的距离度量汇总 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f&title=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 机器学习中常用的距离度量汇总 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 机器学习中常用的距离度量汇总 on whatsapp" href="https://api.whatsapp.com/send?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb%20-%20https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 机器学习中常用的距离度量汇总 on telegram" href="https://telegram.me/share/url?text=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb&url=https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 机器学习中常用的距离度量汇总 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%b8%ad%e5%b8%b8%e7%94%a8%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f%e6%b1%87%e6%80%bb&u=https%3a%2f%2fkwang.life%2f2023%2f08%2f%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E4%25B8%25AD%25E5%25B8%25B8%25E7%2594%25A8%25E7%259A%2584%25E8%25B7%259D%25E7%25A6%25BB%25E5%25BA%25A6%25E9%2587%258F%25E6%25B1%2587%25E6%2580%25BB%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><div id=tcomment></div><script src=https://cdn.staticfile.org/twikoo/1.6.17/twikoo.all.min.js></script>
<script>twikoo.init({envId:'lam-3wx4j3ew',el:'#tcomment'})</script>my comments</article></main><footer class=footer><span>&copy; 2023 <a href=https://kwang.life>退思轩</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><a href="https://ipw.cn/ssl/?site=kwang.life" title=本站支持SSL安全访问 target=_blank><img style=display:inline-block;vertical-align:middle alt=本站支持SSL安全访问 src=https://static.ipw.cn/icon/ssl-s1.svg></a>
<a href="https://ipw.cn/ipv6webcheck/?site=kwang.life" title=本站支持IPv6访问 target=_blank><img style=display:inline-block;vertical-align:middle alt=本站支持IPv6访问 src=https://static.ipw.cn/icon/ipv6-certified-s1.svg></a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>